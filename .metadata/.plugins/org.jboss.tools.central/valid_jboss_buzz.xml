<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Optimizing the Clang compiler’s line-to-offset mapping</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/XzEnFxBRY7M/" /><category term="C" /><category term="clang/LLVM" /><category term="Linux" /><category term="Performance" /><category term="Bit hacks" /><category term="Clang compiler" /><category term="LineOffsetMapping" /><category term="Optimizing Clang" /><author><name>Serge Guelton</name></author><id>https://developers.redhat.com/blog/?p=894827</id><updated>2021-05-04T07:00:38Z</updated><published>2021-05-04T07:00:38Z</published><content type="html">&lt;p&gt;Recently, I&amp;#8217;ve been trying to improve the speed of the &lt;a target="_blank" rel="nofollow" href="/blog/category/clang-llvm/"&gt;Clang compiler&lt;/a&gt; for &lt;a target="_blank" rel="nofollow" href="/topics/c"&gt;C and C++&lt;/a&gt;. When I profile the Clang pre-processing step on a large file, one function quickly stands out:&lt;/p&gt; &lt;pre&gt;clang::LineOffsetMapping::get(llvm::MemoryBufferRef Buffer, llvm::BumpPtrAllocator &amp;#38;Alloc)&lt;/pre&gt; &lt;p&gt;This function basically allocates a vector (through &lt;code&gt;Alloc&lt;/code&gt;) that maps line numbers to offsets in a file (loaded in &lt;code&gt;Buffer&lt;/code&gt;). That&amp;#8217;s a surprisingly standalone function, so it&amp;#8217;s easy to extract it in a micro-benchmark and go for an optimization journey. This article is a kind of &lt;em&gt;log book&lt;/em&gt; of that trip.&lt;/p&gt; &lt;h2&gt;The problem, and the naive implementation&lt;/h2&gt; &lt;p&gt;I describe the purpose of the &lt;code&gt;LineOffsetMapping&lt;/code&gt; function this way:&lt;/p&gt; &lt;blockquote&gt;&lt;p&gt;Given a file loaded in a string &lt;code&gt;Buffer&lt;/code&gt;, create a vector &lt;code&gt;LineOffsets&lt;/code&gt; of size &lt;code&gt;N&lt;/code&gt; where &lt;code&gt;N&lt;/code&gt; is the number of lines in &lt;code&gt;Buffer&lt;/code&gt;, so that &lt;code&gt;LineOffsets[I]&lt;/code&gt; contains the byte offset where that line begins.&lt;/p&gt;&lt;/blockquote&gt; &lt;p&gt;And because cross-platform matters, let&amp;#8217;s add some fun with:&lt;/p&gt; &lt;blockquote&gt;&lt;p&gt;A file can contain any combination of line delimiters including &lt;code&gt;\n&lt;/code&gt;, &lt;code&gt;\r&lt;/code&gt;, and &lt;code&gt;\r\n&lt;/code&gt;.&lt;/p&gt;&lt;/blockquote&gt; &lt;p&gt;A naive implementation—and that&amp;#8217;s actually the one used in clang 12.0.0—could be:&lt;/p&gt; &lt;pre&gt;std::vector&amp;#60;unsigned&amp;#62; LineOffsets; LineOffsets.push_back(0); const unsigned char *Buf = (const unsigned char *)Buffer.data(); const std::size_t BufLen = Buffer.size(); unsigned I = 0; while (I &amp;#60; BufLen) { if (Buf[I] == '\n') { LineOffsets.push_back(I + 1); } else if (Buf[I] == '\r') { // If this is \r\n, skip both characters. if (I + 1 &amp;#60; BufLen &amp;#38;&amp;#38; Buf[I + 1] == '\n') ++I; LineOffsets.push_back(I + 1); } ++I; } &lt;/pre&gt; &lt;p&gt;It&amp;#8217;s simple enough: Read the file, one byte at a time, check its content, and record new lines. But can we do better?&lt;/p&gt; &lt;h2&gt;Manual profile-guided optimization&lt;/h2&gt; &lt;p&gt;Because the input to Clang is source code, the ratio of newline versus non-newline characters will be in favor of the latter. For instance, in the LLVM source code itself:&lt;/p&gt; &lt;pre&gt;$ find llvm -name '*.cpp' -exec cat {} \; | tr -cd '\n' | wc -c 2130902 $ find llvm -name '*.cpp' -exec cat {} \; | wc -c 78537977&lt;/pre&gt; &lt;p&gt;That&amp;#8217;s roughly 2% of newlines.&lt;/p&gt; &lt;p&gt;We can take that property into account using the &lt;code&gt;__builtin_expect(expression, value)&lt;/code&gt; intrinsic to guide the optimization process. We can also take advantage of the coincidence that the two characters between &lt;code&gt;\n&lt;/code&gt; and &lt;code&gt;\r&lt;/code&gt; in the ASCII table—vertical tab (&lt;code&gt;0x0B&lt;/code&gt;) and form feed (&lt;code&gt;0x0C&lt;/code&gt;)—are pretty uncommon, and use a fast check that catches both newline characters:&lt;/p&gt; &lt;pre&gt;unsigned I = 0; while (I &amp;#60; BufLen) { // Use a fast check to catch both newlines if (__builtin_expect((Buf[I] - '\n') &amp;#60;= ('\r' - '\n'), 0)) { if (Buf[I] == '\n') { LineOffsets.push_back(I + 1); } else if (Buf[I] == '\r') { if (I + 1 &amp;#60; BufLen &amp;#38;&amp;#38; Buf[I + 1] == '\n') ++I; LineOffsets.push_back(I + 1); } } ++I; } &lt;/pre&gt; &lt;p&gt;The fuzzy check is going to fail most of the time, and the compiler knows it, which leads to more efficient code.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note&lt;/strong&gt;: It&amp;#8217;s tempting to use an even faster check, &lt;code&gt;Buf[I] &amp;#60;= '\r'&lt;/code&gt;. However, tab characters have the ASCII code &lt;code&gt;0x09&lt;/code&gt; and would be caught by the check. That would be a good punishment for people using tab characters, but not a very gentle move.&lt;/p&gt; &lt;h2&gt;Adding a fast path&lt;/h2&gt; &lt;p&gt;As a &lt;a target="_blank" rel="nofollow" href="/topics/linux"&gt;Linux&lt;/a&gt; user, I find it frustrating to pay for these DOS and OS X-style newlines that never make their way into my codebase. It&amp;#8217;s possible to update the algorithm to do a quick scan for the &lt;code&gt;\r&lt;/code&gt; character, and use a fast path if there is none. If the file does have the character, and uses consistent newline style, the character going to be found very quickly, so the cost should be low:&lt;/p&gt; &lt;pre&gt;if(!memchr(Buf, '\r', BufLen)) { while (I &amp;#60; BufLen) { if (__builtin_expect(Buf[I] == '\n', 0)) { LineOffsets.push_back(I + 1); } ++I; } } else { // one of the version above... or below } &lt;/pre&gt; &lt;h2&gt;Looking through history&lt;/h2&gt; &lt;p&gt;The Clang source code contains an &lt;a href="https://github.com/llvm/llvm-project/blob/release/12.x/clang/lib/Basic/SourceManager.cpp#L1255"&gt;optimization hint&lt;/a&gt;: At some point, &lt;code&gt;clang::LineOffsetMapping::get&lt;/code&gt; had an SSE version, and it got removed for the sake of maintainability in revision &lt;a target="_blank" rel="nofollow" href="https://github.com/llvm/llvm-project/commit/d906e731ece9942260df4097ed8e8b4cbfa70d32"&gt;d906e731&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;This version works hard to use aligned load, which was an important optimization point for older architectures. Since then, the performance cost of unaligned loads on some modern Intel architectures has decreased, so let&amp;#8217;s try an SSE version that&amp;#8217;s easier to read and maintain:&lt;/p&gt; &lt;pre&gt;#include &amp;#60;emmintrin.h&amp;#62; // Some renaming to help the reader not familiar with SSE #define VBROADCAST(v) _mm_set1_epi8(v) #define VLOAD(v) _mm_loadu_si128((const __m128i*)(v)) #define VOR(x, y) _mm_or_si128(x, y) #define VEQ(x, y) _mm_cmpeq_epi8(x, y) #define VMOVEMASK(v) _mm_movemask_epi8(v) const auto LFs = VBROADCAST('\n'); const auto CRs = VBROADCAST('\r'); while (I + sizeof(LFs) + 1 &amp;#60; BufLen) { auto Chunk1 = VLOAD(Buf + I); auto Cmp1 = VOR(VEQ(Chunk1, LFs), VEQ(Chunk1, CRs)); unsigned Mask = VMOVEMASK(Cmp1) ; if(Mask) { unsigned N = __builtin_ctz(Mask); I += N; I += ((Buf[I] == '\r') &amp;#38;&amp;#38; (Buf[I + 1] == '\n'))? 2 : 1; LineOffsets.push_back(I); } else I += sizeof(LFs); } &lt;/pre&gt; &lt;p&gt;In addition to SSE, this version uses the &lt;code&gt;__builtin_ctz&lt;/code&gt; builtin that counts the number of trailing zeroes, and thus allows direct access to the matching character. Because we know that this character is either going to be &lt;code&gt;\n&lt;/code&gt; or &lt;code&gt;\r&lt;/code&gt;, it&amp;#8217;s also possible to avoid a few comparisons and use a one-liner likely to be optimized to a conditional &lt;code&gt;mov&lt;/code&gt; (a.k.a. &lt;code&gt;cmov&lt;/code&gt;) on x86.&lt;/p&gt; &lt;h2&gt;Bit hacks&lt;/h2&gt; &lt;p&gt;The function we&amp;#8217;re trying to craft shares properties with a plain &lt;code&gt;memchr&lt;/code&gt;. Let&amp;#8217;s pay tribute to the ancient and examine the &lt;a target="_blank" rel="nofollow" href="https://github.com/lattera/glibc/blob/master/string/memchr.c#L48"&gt;glibc implementation&lt;/a&gt;. It uses a technique similar to vectorization, but with fewer portability issues (at the price of some brain gymnastics), sometimes called &lt;em&gt;multibyte word packing&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;Bit hack lovers already know the delicious &lt;a target="_blank" rel="nofollow" href="https://en.wikipedia.org/wiki/Hacker%27s_Delight"&gt;Hacker&amp;#8217;s Delight&lt;/a&gt; and its close friend &lt;a target="_blank" rel="nofollow" href="http://graphics.stanford.edu/~seander/bithacks.html"&gt;Bit Twiddling Hacks&lt;/a&gt;. They contain many helpful recipes related to multibyte words, including a way to &lt;a target="_blank" rel="nofollow" href="http://graphics.stanford.edu/~seander/bithacks.html#HasBetweenInWord"&gt;determine if a word has a byte between m and n&lt;/a&gt;. Let&amp;#8217;s apply that technique to our problem:&lt;/p&gt; &lt;pre&gt;template &amp;#60;class T&amp;#62; static constexpr inline T likelyhasbetween(T x, unsigned char m, unsigned char n) { // see http://graphics.stanford.edu/~seander/bithacks.html#HasBetweenInWord return (((x) - ~static_cast&amp;#60;T&amp;#62;(0) / 255 * (n)) &amp;#38; ~(x) &amp;#38; ((x) &amp;#38; ~static_cast&amp;#60;T&amp;#62;(0) / 255 * 127) + ~static_cast&amp;#60;T&amp;#62;(0) / 255 * (127 - (m))) &amp;#38; ~static_cast&amp;#60;T&amp;#62;(0) / 255 * 128; } uint64_t Word; // scan sizeof(Word) bytes at a time for new lines. // This is much faster than scanning each byte independently. if (BufLen &amp;#62; sizeof(Word)) { do { memcpy(&amp;#38;Word, Buf + I, sizeof(Word)); #if defined(BYTE_ORDER) &amp;#38;&amp;#38; defined(BIG_ENDIAN) &amp;#38;&amp;#38; BYTE_ORDER == BIG_ENDIAN Word = __builtin_bswap64(Word); // some endianness love #endif // no new line =&amp;#62; jump over sizeof(Word) bytes. auto Mask = likelyhasbetween(Word, '\n' - 1, '\r'+1 ); if (!Mask) { I += sizeof(Word); continue; } // Otherwise scan for the next newline - it's very likely there's one. // Note that according to // http://graphics.stanford.edu/~seander/bithacks.html#HasBetweenInWord, // likelyhasbetween may have false positive for the upper bound. unsigned N = __builtin_ctzl(Mask) - 7; Word &amp;#62;&amp;#62;= N; I += N / 8 + 1; unsigned char Byte = Word; if (Byte == '\n') { LineOffsets.push_back(I); } else if (Byte == '\r') { // If this is \r\n, skip both characters. if (Buf[I] == '\n') ++I; LineOffsets.push_back(I); } } while (I &amp;#60; BufLen - sizeof(Word) - 1); }&lt;/pre&gt; &lt;p&gt;The code is a bit longer and relies on an undocumented property of the &lt;code&gt;likelyhasbetween&lt;/code&gt; function: It sets the byte holding one of the searched values to &lt;code&gt;0x80&lt;/code&gt;. We can use that marker combined with &lt;code&gt;__builtin_ctzl&lt;/code&gt; (the &lt;code&gt;long&lt;/code&gt; counterpart of the &lt;code&gt;__builtin_ctz&lt;/code&gt; builtin you&amp;#8217;ve already met) to point right at the likely match, just like we did for the SSE version.&lt;/p&gt; &lt;h2&gt;Crunching numbers&lt;/h2&gt; &lt;p&gt;So far, we only explored optimization strategies without doing a single run, profiling, or anything. That&amp;#8217;s of course not how the travel actually took place. I set up a &lt;a target="_blank" rel="nofollow" href="https://github.com/serge-sans-paille/mapping-line-to-offset"&gt;small repo&lt;/a&gt; to gather the various implementations (and a few extras). It provides automation for benchmarking and validating some implementations, using the &lt;a target="_blank" rel="nofollow" href="https://www.sqlite.org/amalgamation.html"&gt;SQLite amalgamation&lt;/a&gt; as input.&lt;/p&gt; &lt;p&gt;Here is the result of the run on my laptop, running a Core i7-8650U and gcc 8.2.1 (timing in milliseconds, average on 100 runs):&lt;/p&gt; &lt;pre&gt;ref: 11.37 seq: 11.12 seq_memchr: 6.53 bithack: 4 bithack_scan: 4.78 sse_align: 5.08 sse: 3 sse_memchr: 3.7 &lt;/pre&gt; &lt;p&gt;&lt;code&gt;ref&lt;/code&gt; is the reference implementation. The extra profiling hint and the fuzzy check in &lt;code&gt;seq&lt;/code&gt; slightly improve the situation, but with a minor margin. The fast path with a &lt;code&gt;memchr&lt;/code&gt; check for &lt;code&gt;\r&lt;/code&gt; yields impressive speedup, but only for one specific case. Two versions of bit hacks are compared, showing that the &lt;code&gt;bithack&lt;/code&gt; version presented in this article is almost as fast as the SSE version. The legacy SSE version that tries to match alignment constraints, &lt;code&gt;sse_align&lt;/code&gt;, does not perform as well as the &lt;code&gt;bithack&lt;/code&gt; version, because matching the alignment has a cost. And trying to use a fast path on the SSE version in &lt;code&gt;sse_memchr&lt;/code&gt; proves to be counterproductive.&lt;/p&gt; &lt;p&gt;Interestingly, but not surprisingly, the configuration matters a lot. On a Mac Mini (on arm64), using the Apple clang version 12.0.0, the results are quite different:&lt;/p&gt; &lt;pre&gt;ref: 6.49 seq: 4 seq_memchr: 4 bithack: 2 bithack_scan: 2.05 &lt;/pre&gt; &lt;p&gt;The call to &lt;code&gt;memchr&lt;/code&gt; doesn&amp;#8217;t have much of an impact there. The profile-guided version from &lt;code&gt;seq&lt;/code&gt; performs significantly better, and both &lt;code&gt;bithack&lt;/code&gt; versions perform three times as fast as the reference version.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;A &lt;a target="_blank" rel="nofollow" href="https://reviews.llvm.org/D99409"&gt;patch has been submitted to LLVM&lt;/a&gt; to use the &lt;code&gt;bithack&lt;/code&gt; version. Although it does not perform as well as the SSE version, the reviewers emphasize the benefit of having an architecture agnostic version that consistently outperforms the reference implementation, and the &lt;code&gt;bithack&lt;/code&gt; version matches these criteria.&lt;/p&gt; &lt;p&gt;For more information on Clang and its LLVM back-end, please visit the &lt;a target="_blank" rel="nofollow" href="/blog/category/clang-llvm/"&gt;Red Hat Developer topic page&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Acknowledgments&lt;/h2&gt; &lt;p&gt;I&amp;#8217;d like to thanks Adrien Guinet for his precious and accurate review of this article &lt;code&gt;o/&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F04%2Foptimizing-the-clang-compilers-line-to-offset-mapping%2F&amp;#38;linkname=Optimizing%20the%20Clang%20compiler%E2%80%99s%20line-to-offset%20mapping" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F04%2Foptimizing-the-clang-compilers-line-to-offset-mapping%2F&amp;#38;linkname=Optimizing%20the%20Clang%20compiler%E2%80%99s%20line-to-offset%20mapping" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F04%2Foptimizing-the-clang-compilers-line-to-offset-mapping%2F&amp;#38;linkname=Optimizing%20the%20Clang%20compiler%E2%80%99s%20line-to-offset%20mapping" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F04%2Foptimizing-the-clang-compilers-line-to-offset-mapping%2F&amp;#38;linkname=Optimizing%20the%20Clang%20compiler%E2%80%99s%20line-to-offset%20mapping" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F04%2Foptimizing-the-clang-compilers-line-to-offset-mapping%2F&amp;#38;linkname=Optimizing%20the%20Clang%20compiler%E2%80%99s%20line-to-offset%20mapping" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F04%2Foptimizing-the-clang-compilers-line-to-offset-mapping%2F&amp;#38;linkname=Optimizing%20the%20Clang%20compiler%E2%80%99s%20line-to-offset%20mapping" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F04%2Foptimizing-the-clang-compilers-line-to-offset-mapping%2F&amp;#38;linkname=Optimizing%20the%20Clang%20compiler%E2%80%99s%20line-to-offset%20mapping" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F04%2Foptimizing-the-clang-compilers-line-to-offset-mapping%2F&amp;#038;title=Optimizing%20the%20Clang%20compiler%E2%80%99s%20line-to-offset%20mapping" data-a2a-url="https://developers.redhat.com/blog/2021/05/04/optimizing-the-clang-compilers-line-to-offset-mapping/" data-a2a-title="Optimizing the Clang compiler’s line-to-offset mapping"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/05/04/optimizing-the-clang-compilers-line-to-offset-mapping/"&gt;Optimizing the Clang compiler’s line-to-offset mapping&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/XzEnFxBRY7M" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Recently, I&amp;#8217;ve been trying to improve the speed of the Clang compiler for C and C++. When I profile the Clang pre-processing step on a large file, one function quickly stands out: clang::LineOffsetMapping::get(llvm::MemoryBufferRef Buffer, llvm::BumpPtrAllocator &amp;#38;Alloc) This function basically allocates a vector (through Alloc) that maps line numbers to offsets in a file (loaded in [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/05/04/optimizing-the-clang-compilers-line-to-offset-mapping/"&gt;Optimizing the Clang compiler’s line-to-offset mapping&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/05/04/optimizing-the-clang-compilers-line-to-offset-mapping/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">894827</post-id><dc:creator>Serge Guelton</dc:creator><dc:date>2021-05-04T07:00:38Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/05/04/optimizing-the-clang-compilers-line-to-offset-mapping/</feedburner:origLink></entry><entry><title>Custom policies in Red Hat 3scale API Management, Part 2: Securing the API with rate limit policies</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/ugn7A0dU_YQ/" /><category term="DevOps" /><category term="Event-Driven" /><category term="Kubernetes" /><category term="3scale API Management" /><category term="APIcast" /><category term="Custom API policies" /><author><name>Satya Jayanti</name></author><id>https://developers.redhat.com/blog/?p=893937</id><updated>2021-05-04T07:00:18Z</updated><published>2021-05-04T07:00:18Z</published><content type="html">&lt;p&gt;In &lt;a target="_blank" rel="nofollow" href="/blog/2021/02/24/custom-policies-in-red-hat-3scale-api-management-part-1-overview/"&gt;Part 1&lt;/a&gt; of this series, we discussed the policy framework in &lt;a target="_blank" rel="nofollow" href="/products/3scale/overview"&gt;Red Hat 3scale API Management&lt;/a&gt;—adding policies to the APIcast gateway to customize API request and response behavior. In this article, we will look at adding rate limiting, backend URL protection, and edge limiting policies to the APIcast gateway. We&amp;#8217;ll also review which policies are appropriate to use for different use cases.&lt;/p&gt; &lt;h2&gt;API gateway as a reverse proxy&lt;/h2&gt; &lt;p&gt;One of the API gateway&amp;#8217;s chief responsibilities is securing the API endpoints. The API gateway acts as a reverse proxy, and all API requests flow through the gateway to the backend APIs. An API exposed through the API gateway is secured in the following ways:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Rate limiting&lt;/strong&gt; controls the number of requests that reach the API by enforcing limits per URL path, method, or user and account plan limits. This is a standard feature of 3scale API Management and is possible using &lt;a target="_blank" rel="nofollow" href="/blog/2021/03/02/packaging-apis-for-consumers-with-red-hat-3scale-api-management/"&gt;API packaging and plans&lt;/a&gt;. You can configure additional policies to limit allowed IP ranges, respond with rate limit headers, and shut down all traffic to the backend during maintenance periods.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Authentication&lt;/strong&gt; provides a way to uniquely identify the requester and only allow access to authenticated accounts. This authentication can happen based on the requester&amp;#8217;s identity. 3scale API Management supports authentication via API (user) keys, application identifier and key pairs, or OpenID Connect (OIDC) based on OAuth 2.0.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Authorization&lt;/strong&gt; lets you manage user and account access based on roles. This goes beyond authentication by looking into the user profile to determine if the user or group should have access to the resource requested. This is configured in 3scale API Management by assigning users and accounts to specific plans. More fine-grained access control can be provided for OIDC-secured services by inspecting the JWT (JSON Web Token) shared by the identity provider and applying role check policies.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In this article, we will primarily focus on the different access control and rate-limiting options available through policies in APIcast.&lt;/p&gt; &lt;h3&gt;Applying the configuration samples&lt;/h3&gt; &lt;p&gt;To apply any of the configuration samples listed in this article, send a PUT request to the Admin API endpoint:&lt;/p&gt; &lt;pre&gt;https://&amp;#60;&amp;#60;3scale admin URL&amp;#62;&amp;#62;/admin/api/services/&amp;#60;&amp;#60;service id&amp;#62;&amp;#62;/proxy/policies.json &lt;/pre&gt; &lt;p&gt;Enter the following information:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Specify the &lt;b&gt;3scale Admin Portal URL&lt;/b&gt; (&lt;code&gt;&amp;#60;&amp;#60;3scale admin URL&amp;#62;&amp;#62;&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Enter the API product&amp;#8217;s &lt;b&gt;service ID number&lt;/b&gt; (&lt;code&gt;&amp;#60;&amp;#60;service id&amp;#62;&amp;#62;&lt;/code&gt;), which can be found on the product overview page in 3scale.&lt;/li&gt; &lt;li&gt;The &lt;b&gt;configuration&lt;/b&gt; JSON can be passed in the body of the request.&lt;/li&gt; &lt;li&gt;The &lt;b&gt;admin access token&lt;/b&gt; can also be passed in the body of the request.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here is an example of the request:&lt;/p&gt; &lt;pre&gt;curl -v  -X PUT   -d 'policies_config=[{"name":"apicast","version":"builtin","configuration":{},"enabled":true}]&amp;#38;access_token=redacted'  https://red-hat-gpte-satya-admin.3scale.net/admin/api/services/18/proxy/policies.json &lt;/pre&gt; &lt;p&gt;If the request is successful, the Admin API sends an HTTP 200 response.&lt;/p&gt; &lt;h2&gt;Anonymous access policy&lt;/h2&gt; &lt;p&gt;In 3scale API Management, you must use one of the three &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_3scale_api_management/2.9/html-single/administering_the_api_gateway/index#supported_authentication_patterns"&gt;authentication methods&lt;/a&gt; to access an API. An &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_3scale_api_management/2.9/html-single/administering_the_api_gateway/index#anonymous_access"&gt;anonymous access policy&lt;/a&gt; lets you bypass this requirement so the API request can be made without providing the authentication in the request.&lt;/p&gt; &lt;p&gt;The policy can be used only if the service is set up to use either the API key method or the &lt;code&gt;App_ID&lt;/code&gt; and &lt;code&gt;App_Key Pair&lt;/code&gt; authentication mechanism. The policy will not work for APIs requiring OIDC authentication.&lt;/p&gt; &lt;p&gt;For the policy, providers need to create an application plan and an application with valid credentials. Using the policy, these credentials can be supplied to the API endpoint during the request if the request does not provide any credentials.&lt;/p&gt; &lt;h3&gt;When to use anonymous access&lt;/h3&gt; &lt;p&gt;You might consider using this policy in the following situations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;When API consumers cannot pass the authentication credentials, as they are legacy or unable to support them.&lt;/li&gt; &lt;li&gt;For testing purposes, so you can reuse existing credentials within the gateway.&lt;/li&gt; &lt;li&gt;In development or staging environments, to make it easier for developers of customer-facing applications to get API access.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please be aware of the following caveats:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The request is anonymous, so combine this policy with an IP check policy (discussed later in this article) to ensure the API endpoint is protected.&lt;/li&gt; &lt;li&gt;Be sure to provide rate limits and deny access to create, update, and delete operations to avoid misuse.&lt;/li&gt; &lt;li&gt;Avoid using this policy in production environments. If necessary, use it as a tactical solution until consumers can be migrated to use authenticated endpoints.&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;How to configure anonymous access&lt;/h3&gt; &lt;p&gt;The anonymous access policy needs to be configured before the APIcast policy in the policy chain. The following is an example of the full configuration:&lt;/p&gt; &lt;pre&gt;[       {         "name": "default_credentials",         "version": "builtin",         "configuration": {           "auth_type": "user_key",           "user_key": "16e66c9c2eee1adb3786221ccffa1e23"         }       },       {         "name": "apicast",         "version": "builtin",         "configuration": {}       }  ]&lt;/pre&gt; &lt;h2&gt;Maintenance mode policy&lt;/h2&gt; &lt;p&gt;Because the APIcast gateway acts as a reverse proxy, all authenticated requests are routed to the backend API. In certain instances, it is possible to prevent the request from reaching the backend by using the &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_3scale_api_management/2.9/html-single/administering_the_api_gateway/index#maintenance-mode"&gt;maintenance mode&lt;/a&gt; policy. This is helpful when the API won&amp;#8217;t accept requests due to scheduled maintenance, or if the backend API is down.&lt;/p&gt; &lt;h3&gt;When to use maintenance mode&lt;/h3&gt; &lt;p&gt;Consider using this policy when:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The backend API is down for maintenance.&lt;/li&gt; &lt;li&gt;You want to provide a more meaningful response to consumers when the API is unavailable or returning an Internal Server Error.&lt;/li&gt; &lt;li&gt;The API is being updated, or API policies are being changed.&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;How to configure maintenance mode&lt;/h3&gt; &lt;p&gt;The maintenance policy needs to be configured before the APIcast policy in the policy chain. The following is an example of the full configuration:&lt;/p&gt; &lt;pre&gt;[       {         "name": "maintenance_mode",         "version": "builtin",         "configuration": {           "message_content_type": "text/plain; charset=utf-8",           "status": 503,           "message": "Service Unavailable - Maintenance. Please try again later."         }       },       {         "name": "apicast",         "version": "builtin",         "configuration": {}       }     ] &lt;/pre&gt; &lt;h2&gt;IP check policy&lt;/h2&gt; &lt;p&gt;API providers require the ability to either allow API calls only from a certain set of pre-configured IPs, or to deny calls from a set of IPs to prevent misuse of the API. By default, the APIcast gateway exposes the API as an HTTP/HTTPS endpoint and does not deny or allow calls based on the IP of the requester. Adding the &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_3scale_api_management/2.9/html-single/administering_the_api_gateway/index#ip_check"&gt;IP check&lt;/a&gt; policy to the policy chain allows you to configure this functionality to the policy chain.&lt;/p&gt; &lt;h3&gt;When to use IP check&lt;/h3&gt; &lt;p&gt;Here are some examples of when you might want to use this policy:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The API is to be allowed for internal customers only, so the API endpoint is only allowed for a set of IPs or IP blocks in the network.&lt;/li&gt; &lt;li&gt;There are a known set of IPs that the provider has blocked for misusing the API or raising malevolent requests, DDoS attacks, etc. The gateway can block such IPs to deny the request without reaching the API.&lt;/li&gt; &lt;li&gt;The API provider wants to provide an IP block to allow a set of partner IPs to access the API, while preventing requesters from outside of the network from accessing it.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please note the APIcast IP check is a layer 7 OSI 7-layer model and is applicable at the TCP/IP layer. The IP check happens at the application layer. For a more robust policy, a LAN network whitelisting or blacklisting policy may be used on the MAC address (layer 2) or firewall policy configured for layers 3 and 4.&lt;/p&gt; &lt;h3&gt;How to configure IP check&lt;/h3&gt; &lt;p&gt;The IP check policy needs to be configured before the APIcast policy in the policy chain.&lt;/p&gt; &lt;p&gt;Use the following configuration and make a request to the Admin API in a POST request:&lt;/p&gt; &lt;pre&gt;[       {         "name": "ip_check",         "version": "builtin",         "configuration": {           "error_msg": "IP address not allowed",           "client_ip_sources": [             "last_caller",             "X-Forwarded-For",             "X-Real-IP"           ],           "ips": [             "1.2.3.4",             "1.2.3.0/4"           ],           "check_type": "whitelist"         }       },       {         "name": "apicast",         "version": "builtin",         "configuration": {}       }     ]&lt;/pre&gt; &lt;h2&gt;Edge limiting policy&lt;/h2&gt; &lt;p&gt;In 3scale, you can control the number of API calls to the backend API by setting limits in the application plans. The rate limit can be set according to the plan type, pricing model, or access control requirements. However, this rate limit does not take into account the backend API&amp;#8217;s throughput capacity. This may lead to overwhelming the API if the number of applications and requests increases with the number of subscribers to the API. An &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_3scale_api_management/2.9/html-single/administering_the_api_gateway/index#edge_limiting"&gt;edge limiting policy&lt;/a&gt; enforces a total number of requests in a given time period for the backend API across all the applications, so that backend API is protected. You can set the rate limits to enforce the number of concurrent requests per second or the number of concurrent connections allowed.&lt;/p&gt; &lt;p&gt;Application plans only enforce rate limits per application. Applications are shared across all the users for a particular account, so you can use the edge limiting policy to allow for user-specific rate limiting by uniquely identifying users through their JWT claim check or request parameters. This ensures no single account user can monopolize the call limit set for the application. Using &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_3scale_api_management/2.9/html-single/administering_the_api_gateway/index#using_variables_and_filters"&gt;liquid templates&lt;/a&gt;, you can set rate limits based on variables like remote IP address, headers, JWT variables, or URL path parameters.&lt;/p&gt; &lt;p&gt;The edge limiting policy uses the OpenResty &lt;a target="_blank" rel="nofollow" href="https://github.com/openresty/lua-resty-limit-traffic"&gt;lua-resty-limit-traffic&lt;/a&gt; library. The policy allows the following limits to be set:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;leaky_bucket_limiters&lt;/code&gt;, based on the leaky bucket algorithm, which builds on the average number of requests plus a maximum burst size.&lt;/li&gt; &lt;li&gt;&lt;code&gt;fixed_window_limiters&lt;/code&gt;, based on a fixed window of time: last n seconds.&lt;/li&gt; &lt;li&gt;&lt;code&gt;connection_limiters&lt;/code&gt;, based on the concurrent number of connections.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Policies normally apply at the gateway level, but the edge limit policy can be service-level and apply to the total number of requests to the backend, irrespective of the number of gateways deployed. An external Redis storage database can be used and configured in case multiple gateways share the edge limit.&lt;/p&gt; &lt;h3&gt;When to use edge limiting&lt;/h3&gt; &lt;p&gt;Here are some examples of when you might consider using this policy:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You want to set an overall limit on backend API across all users, accounts, and applications.&lt;/li&gt; &lt;li&gt;You want to control throughput for popular APIs with hundreds of consumers. For example, if the API can handle 100 concurrent connections, the edge limiting policy can be set accordingly, and it will apply to all applications.&lt;/li&gt; &lt;li&gt;An application plan may have a rate limit of 10 requests per minute, but the number of applications using that plan depends on the number of consumers. If there are 1,000 applications, then theoretically, up to 10,000 requests could be allowed per minute. Setting the edge limit enforces simultaneous usage limits according to API capacity.&lt;/li&gt; &lt;li&gt;An application plan allows for 100 requests per minute, but a single client IP is making 100% of the requests, and other requesters with the same account are unable to access. Setting a limit of 10 requests per minute per client IP ensures the plan is used fairly across the account.&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;How to configure edge limiting&lt;/h3&gt; &lt;p&gt;The edge limiting policy needs to be configured before the APIcast policy in the policy chain. The following examples demonstrate some sample configurations.&lt;/p&gt; &lt;p&gt;To set a concurrent connections rate limit globally with an external Redis storage database:&lt;/p&gt; &lt;pre&gt;[       {         "name": "rate_limit",         "version": "builtin",         "configuration": {           "limits_exceeded_error": {             "status_code": 429,             "error_handling": "exit"           },           "configuration_error": {             "status_code": 500,             "error_handling": "exit"           },           "fixed_window_limiters": [],           "connection_limiters": [             {               "condition": {                 "combine_op": "and"               },               "key": {                 "scope": "service",                 "name_type": "plain"               },               "conn": 100,               "burst": 50,               "delay": 1             }           ],           "redis_url": "redis://gateway-redis:6379/1"         }       },       {         "name": "apicast",         "version": "builtin",         "configuration": {}       }     ]&lt;/pre&gt; &lt;p&gt;To set leaky bucket limiters for a client IP address:&lt;/p&gt; &lt;pre&gt;[       {         "name": "rate_limit",         "version": "builtin",         "configuration": {           "limits_exceeded_error": {             "status_code": 429,             "error_handling": "exit"           },           "configuration_error": {             "status_code": 500,             "error_handling": "exit"           },           "fixed_window_limiters": [],           "connection_limiters": [],           "redis_url": "redis://gateway-redis:6379/1",           "leaky_bucket_limiters": [             {               "condition": {                 "combine_op": "and"               },               "key": {                 "scope": "service",                 "name_type": "liquid",                 "name": "{{ remote_addr }}"               },               "rate": 100,               "burst": 50             }           ]         }       },       {         "name": "apicast",         "version": "builtin",         "configuration": {}       }     ]&lt;/pre&gt; &lt;p&gt;To set a fixed-window limiter for a header match:&lt;/p&gt; &lt;pre&gt;[       {         "name": "rate_limit",         "version": "builtin",         "configuration": {           "limits_exceeded_error": {             "status_code": 429,             "error_handling": "exit"           },           "configuration_error": {             "status_code": 500,             "error_handling": "exit"           },           "fixed_window_limiters": [             {               "window": 10,               "condition": {                 "combine_op": "and"               },               "key": {                 "scope": "service",                 "name_type": "liquid",                 "name": "{{ jwt.sub }}"               },               "count": 100             } r           ]         }       },       {         "name": "apicast",         "version": "builtin",         "configuration": {}       }     ]&lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In this article, we saw how policies can be used to fine-tune the rate limiting and access controls that are set up for the APIcast gateway. In the next article, we will explore using advanced security policies that can work in conjunction with OIDC for providing authorization controls.&lt;/p&gt; &lt;p&gt;You can try out the 3scale API Management platform and create policies, as shown in this article, by &lt;a target="_blank" rel="nofollow" href="/products/3scale/overview"&gt;signing up for free&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F04%2Fcustom-policies-in-red-hat-3scale-api-management-part-2-securing-the-api-with-rate-limit-policies%2F&amp;#38;linkname=Custom%20policies%20in%20Red%20Hat%203scale%20API%20Management%2C%20Part%202%3A%20Securing%20the%20API%20with%20rate%20limit%20policies" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F04%2Fcustom-policies-in-red-hat-3scale-api-management-part-2-securing-the-api-with-rate-limit-policies%2F&amp;#38;linkname=Custom%20policies%20in%20Red%20Hat%203scale%20API%20Management%2C%20Part%202%3A%20Securing%20the%20API%20with%20rate%20limit%20policies" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F04%2Fcustom-policies-in-red-hat-3scale-api-management-part-2-securing-the-api-with-rate-limit-policies%2F&amp;#38;linkname=Custom%20policies%20in%20Red%20Hat%203scale%20API%20Management%2C%20Part%202%3A%20Securing%20the%20API%20with%20rate%20limit%20policies" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F04%2Fcustom-policies-in-red-hat-3scale-api-management-part-2-securing-the-api-with-rate-limit-policies%2F&amp;#38;linkname=Custom%20policies%20in%20Red%20Hat%203scale%20API%20Management%2C%20Part%202%3A%20Securing%20the%20API%20with%20rate%20limit%20policies" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F04%2Fcustom-policies-in-red-hat-3scale-api-management-part-2-securing-the-api-with-rate-limit-policies%2F&amp;#38;linkname=Custom%20policies%20in%20Red%20Hat%203scale%20API%20Management%2C%20Part%202%3A%20Securing%20the%20API%20with%20rate%20limit%20policies" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F04%2Fcustom-policies-in-red-hat-3scale-api-management-part-2-securing-the-api-with-rate-limit-policies%2F&amp;#38;linkname=Custom%20policies%20in%20Red%20Hat%203scale%20API%20Management%2C%20Part%202%3A%20Securing%20the%20API%20with%20rate%20limit%20policies" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F04%2Fcustom-policies-in-red-hat-3scale-api-management-part-2-securing-the-api-with-rate-limit-policies%2F&amp;#38;linkname=Custom%20policies%20in%20Red%20Hat%203scale%20API%20Management%2C%20Part%202%3A%20Securing%20the%20API%20with%20rate%20limit%20policies" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F04%2Fcustom-policies-in-red-hat-3scale-api-management-part-2-securing-the-api-with-rate-limit-policies%2F&amp;#038;title=Custom%20policies%20in%20Red%20Hat%203scale%20API%20Management%2C%20Part%202%3A%20Securing%20the%20API%20with%20rate%20limit%20policies" data-a2a-url="https://developers.redhat.com/blog/2021/05/04/custom-policies-in-red-hat-3scale-api-management-part-2-securing-the-api-with-rate-limit-policies/" data-a2a-title="Custom policies in Red Hat 3scale API Management, Part 2: Securing the API with rate limit policies"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/05/04/custom-policies-in-red-hat-3scale-api-management-part-2-securing-the-api-with-rate-limit-policies/"&gt;Custom policies in Red Hat 3scale API Management, Part 2: Securing the API with rate limit policies&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/ugn7A0dU_YQ" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;In Part 1 of this series, we discussed the policy framework in Red Hat 3scale API Management—adding policies to the APIcast gateway to customize API request and response behavior. In this article, we will look at adding rate limiting, backend URL protection, and edge limiting policies to the APIcast gateway. We&amp;#8217;ll also review which policies [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/05/04/custom-policies-in-red-hat-3scale-api-management-part-2-securing-the-api-with-rate-limit-policies/"&gt;Custom policies in Red Hat 3scale API Management, Part 2: Securing the API with rate limit policies&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/05/04/custom-policies-in-red-hat-3scale-api-management-part-2-securing-the-api-with-rate-limit-policies/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">893937</post-id><dc:creator>Satya Jayanti</dc:creator><dc:date>2021-05-04T07:00:18Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/05/04/custom-policies-in-red-hat-3scale-api-management-part-2-securing-the-api-with-rate-limit-policies/</feedburner:origLink></entry><entry><title>Event-driven APIs and schema governance for Apache Kafka: Get ready for Kafka Summit Europe 2021</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/TOjaNK_7AgY/" /><category term="Containers" /><category term="Event-Driven" /><category term="Kubernetes" /><category term="Microservices" /><category term="Apache Kafka" /><category term="Apicurio" /><category term="contract-first development" /><category term="Event-driven APIs" /><author><name>Hugo Guerrero</name></author><id>https://developers.redhat.com/blog/?p=899387</id><updated>2021-05-04T07:00:04Z</updated><published>2021-05-04T07:00:04Z</published><content type="html">&lt;p&gt;As a developer, I’m always excited to attend the &lt;a target="_blank" rel="nofollow" href="https://www.kafka-summit.org/"&gt;Kafka Summit&lt;/a&gt;, happening this year from May 11 to 12. There are so many great sessions addressing critical challenges in the &lt;a target="_blank" rel="nofollow" href="https://kafka.apache.org/"&gt;Apache Kafka&lt;/a&gt; ecosystem. One example is how changes to event-driven APIs are leading developers to focus on &lt;a target="_blank" rel="nofollow" href="https://www.redhat.com/en/blog/achieving-promise-microservices-one-contract-time"&gt;contract-first development&lt;/a&gt; for Kafka.&lt;/p&gt; &lt;p&gt;In preparation for the upcoming Kafka Summit, this article discusses the journey Kafka users have taken to get on the API bandwagon and how developers are using contracts to describe brokers without losing control of their data in the cluster. A critical component for effective schema governance is having a schema registry such as &lt;a target="_blank" rel="nofollow" href="/blog/2020/06/11/first-look-at-the-new-apicurio-registry-ui-and-operator/"&gt;Apicurio Registry&lt;/a&gt;. See the end of the article for information about Red Hat’s sessions during the Kafka Summit Europe 2021.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note&lt;/strong&gt;: &lt;em&gt;Contract-first development&lt;/em&gt; is a design approach where business users or developers agree on a service&amp;#8217;s desired structure and result upfront, before sharing it with the development team.&lt;/p&gt; &lt;h2&gt;Distributed, decoupled, and highly connected systems&lt;/h2&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="/blog/category/modern-app-dev/"&gt;Modern application development&lt;/a&gt; has shifted toward distributed, decoupled, and highly connected systems over the past several years. Distributed applications involve deployments across multiple data centers, cloud providers, and geographical regions. Multiple teams working on different applications require the components to be decoupled in terms of technology, development, and even time. The last type of decoupling is critical for systems that produce events and data changes to be consumed by applications at a future time. In some cases, the application is not online when the change is made. In other cases, the application that will consume the events and data doesn&amp;#8217;t even exist yet. Finally, no application lives in a vacuum: Every application must be appropriately and efficiently connected to other components or legacy applications.&lt;/p&gt; &lt;p&gt;In response to these demands, we have seen the rise of &lt;a target="_blank" rel="nofollow" href="/topics/microservices"&gt;microservices&lt;/a&gt;, HTTP-based applications using a request-and-response pattern. Microservices offer a better way for teams to develop their domain services and applications without impacting other groups. REST APIs simplify connections between applications using the well-understood, debuggable HTTP protocol. Simultaneously, we&amp;#8217;ve seen the revival of &lt;a target="_blank" rel="nofollow" href="/topics/event-driven"&gt;event-driven architectures&lt;/a&gt;, powered by the rise of modern message brokers. All of these factors have converged in a turn toward democratized processing patterns such as event stream analytics.&lt;/p&gt; &lt;h2&gt;Bottlenecks in the code-first approach&lt;/h2&gt; &lt;p&gt;Implementing an event-driven architecture using &lt;a target="_blank" rel="nofollow" href="/topics/kafka-kubernetes"&gt;Apache Kafka&lt;/a&gt; alongside the traditional API approach has brought new challenges and expectations. The conventional code-first workflow (of implementing the code first and then sharing the resulting API specification) includes many bottlenecks that prevent efficient progress. Developers are seeking a new direction for discoverability and access to event-stream endpoints.&lt;/p&gt; &lt;p&gt;The code-first process leads to questions about how to address the relationship between event providers and consumers. From a developer&amp;#8217;s perspective, the Apache Kafka endpoint must be clearly documented. The documentation should include information about the Kafka bootstrap server or the cluster’s security model, such as authentication. In this way, the documentation becomes a description of your Kafka brokers.&lt;/p&gt; &lt;p&gt;On the other hand, Apache Kafka transfers the responsibility for processing data structures to clients. This delegation allows Kafka to handle high throughput and be highly scalable. But it also requires that developers are aware of the data format and data validation when sending or receiving data from Kafka topics. It&amp;#8217;s easy to share this information through informal channels within small teams, but when you increase the reach to external users and third parties, it can become a nightmare. Addressing these issues is an important challenge.&lt;/p&gt; &lt;h2&gt;Event-driven APIs and contract-first workflows&lt;/h2&gt; &lt;p&gt;Having a simple format for sharing contracts for your services among all of its users is highly valuable. The contract-first process creates the contract for the service beforehand. In this way, producers and consumers know upfront what to expect from the service provider.&lt;/p&gt; &lt;p&gt;Knowing the endpoint definition in advance allows developers to work independently. It also ensures consistency between both parties. It provides strong guarantees about service contracts, so that teams, users, and partners can collaborate more effectively. The contract-first approach also lets developers save time by using code generators and testing tools.&lt;/p&gt; &lt;p&gt;Distributed services using REST APIs have promoted the contract-first workflow by consolidating the &lt;a target="_blank" rel="nofollow" href="https://swagger.io/specification/"&gt;OpenAPI&lt;/a&gt; specification to model their endpoints. The rallying around this format helped create a new practice of managing &lt;a href="https://developers.redhat.com/blog/2019/12/03/apis-as-a-product-get-started-in-no-time/"&gt;API contracts as products&lt;/a&gt; by themselves. As a result, specification editors such as &lt;a target="_blank" rel="nofollow" href="https://swagger.io/tools/swaggerhub/"&gt;SwaggerHub&lt;/a&gt; and &lt;a target="_blank" rel="nofollow" href="/blog/tag/apicurio/"&gt;Apicurio&lt;/a&gt;, OpenAPI code generators, and additional tooling have become available. Going further, the specification document can even help to mock services using platforms like &lt;a target="_blank" rel="nofollow" href="/blog/tag/microcks/"&gt;Microcks&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Another secret weapon for handling contract-first agreements is the &lt;a target="_blank" rel="nofollow" href="https://www.asyncapi.com/"&gt;AsyncAPI specification&lt;/a&gt;. AsyncAPI is an &lt;a target="_blank" rel="nofollow" href="/topics/open-source"&gt;open source&lt;/a&gt; initiative that seeks to improve the current state of event-driven architectures. The AsyncAPI specification describes event-driven APIs using messaging technologies like &lt;a target="_blank" rel="nofollow" href="/blog/2021/04/16/deploying-the-mosquitto-mqtt-message-broker-on-red-hat-openshift-part-1/"&gt;MQTT&lt;/a&gt;, &lt;a target="_blank" rel="nofollow" href="/products/amq/overview"&gt;AMQP&lt;/a&gt;, and Apache Kafka. It started as a sister specification to OpenAPI, using the same syntax and &lt;a target="_blank" rel="nofollow" href="https://json-schema.org/"&gt;JSON Schema&lt;/a&gt; under the hood. Like any contract defined with OpenAPI, AsyncAPI helps you achieve visibility and agreement for your event-driven APIs.&lt;/p&gt; &lt;h2&gt;AsyncAPI: Schemas as event contracts&lt;/h2&gt; &lt;p&gt;If you are &lt;a target="_blank" rel="nofollow" href="https://www.asyncapi.com/docs/getting-started/coming-from-openapi"&gt;familiar with OpenAPI&lt;/a&gt;, you will easily recognize the similarities between the AsyncAPI and OpenAPI specifications. The server information is still there, paths become &lt;em&gt;channels&lt;/em&gt;, and operations are simplified to &lt;em&gt;publish&lt;/em&gt; and &lt;em&gt;subscribe&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;However, the most crucial part is the &lt;em&gt;message section&lt;/em&gt;. This section defines the content type and structure of a message&amp;#8217;s headers and payloads. Developers coming from the Kafka world will see that, in practice, this part of the specification is for defining the messages’ schema. The message section can also refer to schemas of different types, such as an &lt;a target="_blank" rel="nofollow" href="http://avro.apache.org/"&gt;Apache Avro&lt;/a&gt; or JSON Schema. It can also include examples of payloads and headers.&lt;/p&gt; &lt;p&gt;AsyncAPI reflects the need to express the Kafka notion of schemas for the data sent and received from Kafka, tackling one of the producer’s and consumer’s challenges in different teams. In the path to event-driven APIs, schemas have become the contracts used for events. For this reason, the same benefits that the contract-first strategy derives from conventional REST APIs apply to event-driven APIs.&lt;/p&gt; &lt;p&gt;To better understand the value of this approach, consider the following scenario: A producer sends data to Kafka, and a consumer retrieves the data. However, Kafka’s asynchronous communication does not allow the producer to know who will consume the data, or when. Also, because the data stored in Kafka topics is in a shared state, new clients can process the data. If the producer team makes a breaking change in the record structure, they might reach out to the original consumer team about the change. But new clients added later will likely miss that information, so they&amp;#8217;ll keep using their old deserialization process. As a result, the execution will break when the latest events arrive, as shown in Figure 1. All of this is why a central registry where data schemas are stored and made available for consumers is critical to effective governance.&lt;/p&gt; &lt;div id="attachment_899407" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/04/schema-evolution.png"&gt;&lt;img aria-describedby="caption-attachment-899407" class="wp-image-899407 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2021/04/schema-evolution-1024x481.png" alt="A change by the producer to the schema can create compatibility problems for consumers." width="640" height="301" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/04/schema-evolution-1024x481.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2021/04/schema-evolution-300x141.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/04/schema-evolution-768x361.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2021/04/schema-evolution.png 1476w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-899407" class="wp-caption-text"&gt;Figure 1: A change to the schema creates a compatibility problem.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Apicurio: A registry for event schemas&lt;/h2&gt; &lt;p&gt;One common way to solve the schema governance problem is to use a &lt;em&gt;registry&lt;/em&gt;. This registry needs to address three main capabilities for successfully managing schemas: First, the registry needs to manage artifacts comprehensively. It must include the ability to store, browse, retrieve, and search the managed items. Second, it must support different data structures, like Apache Avro, Google Protocol Buffers, and JSON Schema. Finally, the registry should use rules about validity and compatibility to follow and control the evolution of the artifact’s content.&lt;/p&gt; &lt;p&gt;If the registry implements these capabilities, it can become the central repository for schemas where producers and consumers share their data structure and Kafka topics. However, following the suggested approach for contract-first development, the registry should not be only for schemas. The registry should also store the definition of the &lt;em&gt;endpoint&lt;/em&gt; and &lt;em&gt;channel&lt;/em&gt;, as implemented by the Kafka brokers and topics. In this way, you can provide a streamlined experience for developers.&lt;/p&gt; &lt;p&gt;Coming back to our example from Figure 1, let&amp;#8217;s say that the producer client’s serializer code could reach a registry API and query for the schema to use to encode the new data. There are several ways to feed schemas to the registry, from the producer doing self-registration to another team managing the schemas. After retrieving the correct schema, the producer could use it to serialize the data in the expected format and send it to Kafka. Finally, the consumer would use the same strategy to retrieve the schema from the registry to deserialize the data, as shown in Figure 2.&lt;/p&gt; &lt;div id="attachment_899417" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/04/registry-in-action.png"&gt;&lt;img aria-describedby="caption-attachment-899417" class="wp-image-899417 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2021/04/registry-in-action-1024x449.png" alt="The producer (serializer) registers or gets a schema before producing data in that schema, and the consumer (deserializer) gets the schema before receiving the data." width="640" height="281" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/04/registry-in-action-1024x449.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2021/04/registry-in-action-300x132.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/04/registry-in-action-768x337.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-899417" class="wp-caption-text"&gt;Figure 2: A registry for schemas in action.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;The shift in modern application development has pulled event-driven architecture back into the spotlight. Kafka is undoubtedly on the rise and is now ubiquitous. The evolution of APIs allows developers to evolve from traditional messaging systems to fully enabled, event-driven APIs.&lt;/p&gt; &lt;p&gt;Adopting the contract-first model is critical for easing this evolutionary process. In a contract-first approach, you can use the AsyncAPI to define access to your Kafka brokers and topics in the same way you have used OpenAPI to define HTTP endpoints. The Kafka record schema becomes an &lt;em&gt;event contract&lt;/em&gt;, and as such, needs to be managed as a product in itself. Finally, having a registry for schemas helps you with schema governance.&lt;/p&gt; &lt;p&gt;As a schema registry, Apicurio Registry is an end-to-end solution for storing API definitions and schemas for Kafka applications. The project includes serializers, deserializers, and additional tooling. The registry supports several types of artifacts, including OpenAPI, AsyncAPI, GraphQL, Apache Avro, Google Protocol Buffers, JSON, Kafka Connect, WSDL, and XML Schema (XSD). Apicurio also checks the validity and compatibility rules for these artifacts.&lt;/p&gt; &lt;p&gt;For developers who want an open source development model with enterprise support, &lt;a target="_blank" rel="nofollow" href="/integration"&gt;Red Hat Integration&lt;/a&gt; lets you deploy your Kafka-based event-driven architecture on &lt;a target="_blank" rel="nofollow" href="/products/openshift/overview"&gt; Red Hat OpenShift&lt;/a&gt;, the enterprise &lt;a target="_blank" rel="nofollow" href="/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;. &lt;a target="_blank" rel="nofollow" href="/products/amq/overview"&gt;Red Hat AMQ Streams&lt;/a&gt;, &lt;a target="_blank" rel="nofollow" href="https://debezium.io/"&gt;Debezium&lt;/a&gt;, and the &lt;a target="_blank" rel="nofollow" href="https://camel.apache.org/camel-kafka-connector/latest/"&gt;Apache Camel Kafka Connector&lt;/a&gt; are all available with a Red Hat Integration subscription.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note&lt;/strong&gt;: We&amp;#8217;ve just &lt;a target="_blank" rel="nofollow" href="https://www.redhat.com/en/blog/introducing-red-hat-openshift-streams-apache-kafka"&gt;announced&lt;/a&gt; the new fully managed service with &lt;a target="_blank" rel="nofollow" href="/products/rhosak/overview"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt;. Get started now with a &lt;a target="_blank" rel="nofollow" href="/products/rhosak/getting-started"&gt;free Apache Kafka cluster&lt;/a&gt;!&lt;/p&gt; &lt;h2&gt;Join us at Kafka Summit Europe 2021&lt;/h2&gt; &lt;p&gt;To help you learn more about achieving schema governance and getting started with event-driven APIs based on Apache Kafka, Red Hat is sponsoring Kafka Summit Europe 2021 from May 11 to 12, 2021. This year&amp;#8217;s summit includes many great topics and speakers, including the following breakouts:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;b&gt;May 12 at 10:30 a.m. GMT&lt;/b&gt;: &lt;a target="_blank" rel="nofollow" href="https://www.kafka-summit.org/sessions/advanced-change-data-streaming-patterns-in-distributed-systems"&gt;Advanced Change Data Streaming Patterns in Distributed Systems&lt;/a&gt;: Gunnar Morling and Hans-Peter Grahsl show you how to do change data capture the open source way with Debezium.&lt;/li&gt; &lt;li&gt;&lt;b&gt;May 12 at 11:00 a.m. GMT&lt;/b&gt;: &lt;a target="_blank" rel="nofollow" href="https://www.kafka-summit.org/sessions/apicurio-registry-event-driven-apis-and-schema-governance-for-apache-kafka"&gt;Apicurio Registry: Event-driven APIs &amp;#38; Schema Governance for Apache Kafka&lt;/a&gt;: Fabian Martinez and Hugo Guerrero demonstrate how to use Apicurio Registry as a registry for schemas.&lt;/li&gt; &lt;li&gt;&lt;b&gt;May 12 at 1:30 p.m. GMT&lt;/b&gt;: &lt;a target="_blank" rel="nofollow" href="https://www.kafka-summit.org/sessions/everything-you-ever-needed-to-know-about-kafka-on-kubernetes-but-were-afraid"&gt;Everything You Ever Needed To Know About Kafka on Kubernetes But Were Afraid To Ask&lt;/a&gt;: Jakub Scholz discusses the resources required to run Apache Kafka on Kubernetes.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can always stop by the Red Hat virtual booth and see what&amp;#8217;s new. We&amp;#8217;ll be glad to chat with you! Look for these topics during our hosted office hours on both days of the summit:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;b&gt;12:00 p.m. to 1:00 p.m. GMT&lt;/b&gt;: Debezium demos and office hours by the Debezium Project team.&lt;/li&gt; &lt;li&gt;&lt;b&gt;2:00 p.m. to 3:00 p.m. GMT&lt;/b&gt;: Registry demos and office hours by the Apicurio project team.&lt;/li&gt; &lt;li&gt;&lt;b&gt;4:00 p.m. to 6:00 p.m. GMT&lt;/b&gt;: A &lt;a target="_blank" rel="nofollow" href="https://strimzi.io/"&gt;Strimzi&lt;/a&gt; demo and office hours by the Strimzi project team.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F04%2Fevent-driven-apis-and-schema-governance-for-apache-kafka-get-ready-for-kafka-summit-europe-2021%2F&amp;#38;linkname=Event-driven%20APIs%20and%20schema%20governance%20for%20Apache%20Kafka%3A%20Get%20ready%20for%20Kafka%20Summit%20Europe%202021" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F04%2Fevent-driven-apis-and-schema-governance-for-apache-kafka-get-ready-for-kafka-summit-europe-2021%2F&amp;#38;linkname=Event-driven%20APIs%20and%20schema%20governance%20for%20Apache%20Kafka%3A%20Get%20ready%20for%20Kafka%20Summit%20Europe%202021" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F04%2Fevent-driven-apis-and-schema-governance-for-apache-kafka-get-ready-for-kafka-summit-europe-2021%2F&amp;#38;linkname=Event-driven%20APIs%20and%20schema%20governance%20for%20Apache%20Kafka%3A%20Get%20ready%20for%20Kafka%20Summit%20Europe%202021" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F04%2Fevent-driven-apis-and-schema-governance-for-apache-kafka-get-ready-for-kafka-summit-europe-2021%2F&amp;#38;linkname=Event-driven%20APIs%20and%20schema%20governance%20for%20Apache%20Kafka%3A%20Get%20ready%20for%20Kafka%20Summit%20Europe%202021" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F04%2Fevent-driven-apis-and-schema-governance-for-apache-kafka-get-ready-for-kafka-summit-europe-2021%2F&amp;#38;linkname=Event-driven%20APIs%20and%20schema%20governance%20for%20Apache%20Kafka%3A%20Get%20ready%20for%20Kafka%20Summit%20Europe%202021" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F04%2Fevent-driven-apis-and-schema-governance-for-apache-kafka-get-ready-for-kafka-summit-europe-2021%2F&amp;#38;linkname=Event-driven%20APIs%20and%20schema%20governance%20for%20Apache%20Kafka%3A%20Get%20ready%20for%20Kafka%20Summit%20Europe%202021" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F04%2Fevent-driven-apis-and-schema-governance-for-apache-kafka-get-ready-for-kafka-summit-europe-2021%2F&amp;#38;linkname=Event-driven%20APIs%20and%20schema%20governance%20for%20Apache%20Kafka%3A%20Get%20ready%20for%20Kafka%20Summit%20Europe%202021" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F04%2Fevent-driven-apis-and-schema-governance-for-apache-kafka-get-ready-for-kafka-summit-europe-2021%2F&amp;#038;title=Event-driven%20APIs%20and%20schema%20governance%20for%20Apache%20Kafka%3A%20Get%20ready%20for%20Kafka%20Summit%20Europe%202021" data-a2a-url="https://developers.redhat.com/blog/2021/05/04/event-driven-apis-and-schema-governance-for-apache-kafka-get-ready-for-kafka-summit-europe-2021/" data-a2a-title="Event-driven APIs and schema governance for Apache Kafka: Get ready for Kafka Summit Europe 2021"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/05/04/event-driven-apis-and-schema-governance-for-apache-kafka-get-ready-for-kafka-summit-europe-2021/"&gt;Event-driven APIs and schema governance for Apache Kafka: Get ready for Kafka Summit Europe 2021&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/TOjaNK_7AgY" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;As a developer, I’m always excited to attend the Kafka Summit, happening this year from May 11 to 12. There are so many great sessions addressing critical challenges in the Apache Kafka ecosystem. One example is how changes to event-driven APIs are leading developers to focus on contract-first development for Kafka. In preparation for the [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/05/04/event-driven-apis-and-schema-governance-for-apache-kafka-get-ready-for-kafka-summit-europe-2021/"&gt;Event-driven APIs and schema governance for Apache Kafka: Get ready for Kafka Summit Europe 2021&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/05/04/event-driven-apis-and-schema-governance-for-apache-kafka-get-ready-for-kafka-summit-europe-2021/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">899387</post-id><dc:creator>Hugo Guerrero</dc:creator><dc:date>2021-05-04T07:00:04Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/05/04/event-driven-apis-and-schema-governance-for-apache-kafka-get-ready-for-kafka-summit-europe-2021/</feedburner:origLink></entry><entry><title>Instant replay: Debugging C and C++ programs with rr</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/k-l5ALdtZH4/" /><category term="C" /><category term="Developer Tools" /><category term="Linux" /><category term="C/C++ debugger" /><category term="gdb" /><category term="GNU Debugger" /><category term="rr" /><author><name>William Cohen</name></author><id>https://developers.redhat.com/blog/?p=862007</id><updated>2021-05-03T07:00:45Z</updated><published>2021-05-03T07:00:45Z</published><content type="html">&lt;p&gt;The common theme in many time-travel movies is to go back in time to find out what went wrong and fix it. Developers also have that desire to go back in time and find why the code broke and fix it. But, often, that crucial step where everything went wrong happened long ago, and the information is no longer available.&lt;/p&gt; &lt;p&gt;The &lt;a target="_blank" rel="nofollow" href="https://rr-project.org/"&gt;rr project&lt;/a&gt; lets programmers examine the entire life of a &lt;a target="_blank" rel="nofollow" href="/topics/c"&gt;C or C++&lt;/a&gt; program run, and replay code execution to see what action in the past caused &amp;#8220;things to go horribly wrong.&amp;#8221; &lt;code&gt;rr&lt;/code&gt; is packaged with &lt;a target="_blank" rel="nofollow" href="https://fedoraproject.org/wiki/EPEL"&gt;Extra Packages for Enterprise Linux (EPEL)&lt;/a&gt; in &lt;a target="_blank" rel="nofollow" href="/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL), and with Fedora 31, 32, 33, and 34.&lt;/p&gt; &lt;p&gt;&lt;code&gt;rr&lt;/code&gt; records trace information about the execution of an application. This information allows you to repeatedly replay a particular recording of a failure and examine it in the &lt;a target="_blank" rel="nofollow" href="https://www.gnu.org/software/gdb/"&gt;GNU Debugger&lt;/a&gt; (GDB) to better investigate the cause. In addition to replaying the trace, &lt;code&gt;rr&lt;/code&gt; lets you run the program in reverse, in essence allowing you &amp;#8220;rewind the tape&amp;#8221; to see what happened earlier in the execution of the program.&lt;/p&gt; &lt;p&gt;The techniques that &lt;code&gt;rr&lt;/code&gt; provides for recording the reproducer for further examination can be a useful addition to traditional core dumps and backtraces, which give a snapshot of an issue at  a particular moment. The &lt;code&gt;rr&lt;/code&gt; recording can provide a way for developers to further investigate intermittent problems where only some application runs fail.&lt;/p&gt; &lt;p&gt;Let&amp;#8217;s see how to set up &lt;code&gt;rr&lt;/code&gt; and use it in an example to better illustrate its utility.&lt;/p&gt; &lt;h2&gt;Requirements and setup&lt;/h2&gt; &lt;p&gt;Because &lt;code&gt;rr&lt;/code&gt; uses a number of newer Linux kernel features and specific processor performance monitoring hardware, the environments it runs in are limited. The newer Fedora kernels have the needed support. However, to correctly track and recreate when asynchronous events happen in a thread, &lt;code&gt;rr&lt;/code&gt;uses performance monitoring hardware event counters that provide deterministic counts of when a thread is interrupted by an asynchronous event. Currently, &lt;code&gt;rr&lt;/code&gt; supports these counts only for Intel processors using the Westmere or newer microarchitectures.&lt;/p&gt; &lt;p&gt;Installing &lt;code&gt;rr&lt;/code&gt; on Fedora is a simple task, requiring a single RPM. If you are on RHEL, you will need to enable EPEL. Once you have access to the appropriate repositories, you can install &lt;code&gt;rr&lt;/code&gt; with the following command:&lt;/p&gt; &lt;pre&gt;$ sudo dnf -y install rr&lt;/pre&gt; &lt;p&gt;Due to the possibility that hardware performance monitoring counters might leak privileged information, many kernels default to limiting their monitoring capabilities. You will need to run the following command to allow &lt;code&gt;rr&lt;/code&gt; access to the performance monitoring counters:&lt;/p&gt; &lt;pre&gt;$ sudo sh -c 'echo 1 &amp;#62;/proc/sys/kernel/perf_event_paranoid'&lt;/pre&gt; &lt;p&gt;If you want to make that setting for the performance monitoring hardware permanent, you can also add the following line to &lt;code&gt;/etc/sysctl.conf&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;kernel.perf_event_paranoid = 1&lt;/pre&gt; &lt;h2&gt;A simple debugging example&lt;/h2&gt; &lt;p&gt;The following is a toy program that computes 2 times 0, 1, 2, and 3 in an array and prints the information:&lt;/p&gt; &lt;pre&gt;#include &amp;#60;stdio.h&amp;#62; #define SIZE 4 void zero(char *a, int size) { while (size&amp;#62;0) a[size--] = 0; } void initialize(char *a, int size) { zero(a, size); } void multiply(char *a, int size, int mult) { int i; for (i=0; i&amp;#60;size; i++) a[i] = i * mult; } void pr_array(char *a, int size) { int i; for (i=0; i&amp;#60;size; i++) printf("f(%d)=%d\n", i, a[i]); } int main(int argc, char **argv) { char a[SIZE]; int mult = 2; initialize(a, SIZE); multiply(a, SIZE, mult); pr_array(a, SIZE); return 0; } &lt;/pre&gt; &lt;p&gt;Compile it with the usual command, as follows, and include the debug option &lt;code&gt;-g&lt;/code&gt; to allow later debugging:&lt;/p&gt; &lt;pre&gt;$ gcc -g multiply.c -o multiply&lt;/pre&gt; &lt;p&gt;When you run &lt;code&gt;multiply&lt;/code&gt;, the results may be very surprising. All the results are zero. The main function is passing in 2 to the &lt;code&gt;multiply&lt;/code&gt; function. The loop in that function is very straightforward. How could this possibly go wrong?&lt;/p&gt; &lt;pre&gt;$ ./multiply f(0)=0 f(1)=0 f(2)=0 f(3)=0&lt;/pre&gt; &lt;h3&gt;Recording the issue&lt;/h3&gt; &lt;p&gt;You can investigate what is going wrong with &lt;code&gt;rr&lt;/code&gt;. Record a run of the &lt;code&gt;multiply&lt;/code&gt; program that demonstrates the issue with the following command. If the problem is intermittent, you could make multiple runs until the issue is seen:&lt;/p&gt; &lt;pre&gt;$ rr record ./multiply rr: Saving execution to trace directory `/home/wcohen/.local/share/rr/multiply-0`. f(0)=0 f(1)=0 f(2)=0 f(3)=0 &lt;/pre&gt; &lt;h3&gt;Replaying and investigating the issue&lt;/h3&gt; &lt;p&gt;To start debugging the issue, use the &lt;code&gt;rr replay&lt;/code&gt; command, which puts you into a GDB session:&lt;/p&gt; &lt;pre&gt;$ rr replay &lt;/pre&gt; &lt;p&gt;You are at the start of execution:&lt;/p&gt; &lt;pre&gt;(rr) where #0 0x00007f7f56b1f110 in _start () from /lib64/ld-linux-x86-64.so.2 #1 0x0000000000000001 in ?? () #2 0x00007ffe0bea6889 in ?? () #3 0x0000000000000000 in ?? () &lt;/pre&gt; &lt;p&gt;You can continue the program from the start and see that it has the same results as before:&lt;/p&gt; &lt;pre&gt;(rr) c Continuing. f(0)=0 f(1)=0 f(2)=0 f(3)=0 Program received signal SIGKILL, Killed. 0x0000000070000002 in ?? () &lt;/pre&gt; &lt;p&gt;You can set a breakpoint at &lt;code&gt;return 0&lt;/code&gt; in the &lt;code&gt;main&lt;/code&gt; function and work backward from there:&lt;/p&gt; &lt;pre&gt;(rr) break multiply.c:37 Breakpoint 1 at 0x401258: file multiply.c, line 37. (rr) c Continuing. f(0)=0 f(1)=0 f(2)=0 f(3)=0 Breakpoint 1, main (argc=1, argv=0x7ffe0bea5c58) at multiply.c:37 37 return 0; &lt;/pre&gt; &lt;p&gt;First, we check the values in the array &lt;code&gt;a&lt;/code&gt;. Maybe the &lt;code&gt;pr_array&lt;/code&gt; function printed wrong values. But that isn&amp;#8217;t the problem, because according to GDB, the values are all 0:&lt;/p&gt; &lt;pre&gt;(rr) print a[0] $5 = 0 '\000' (rr) print a[1] $6 = 0 '\000' (rr) print a[2] $7 = 0 '\000' (rr) print a[3] $8 = 0 '\000' &lt;/pre&gt; &lt;p&gt;Maybe &lt;code&gt;pr_array&lt;/code&gt; corrupted the values. Let&amp;#8217;s set a breakpoint on the entry to &lt;code&gt;pr_array&lt;/code&gt; and go backward in execution with a &lt;code&gt;reverse-continue&lt;/code&gt; command, to see what the state was before the &lt;code&gt;pr_array&lt;/code&gt; function executed. Looks like &lt;code&gt;pr_array&lt;/code&gt; is printing the correct values:&lt;/p&gt; &lt;pre&gt;(rr) break pr_array Breakpoint 2 at 0x4011cc: file multiply.c, line 25. (rr) reverse-continue Continuing. Breakpoint 2, pr_array (a=0x7ffe0bea5b58 "", size=4) at multiply.c:25 25 for (i=0; i&amp;#60;size; i++) (rr) print a[0] $9 = 0 '\000' (rr) print a[1] $10 = 0 '\000' (rr) print a[2] $11 = 0 '\000' (rr) print a[3] $12 = 0 '\000' &lt;/pre&gt; &lt;p&gt;Maybe something is going wrong in the &lt;code&gt;multiply&lt;/code&gt; function. Let&amp;#8217;s set a breakpoint on it and &lt;code&gt;reverse-continue&lt;/code&gt; to it:&lt;/p&gt; &lt;pre&gt;rr) break multiply Breakpoint 3 at 0x401184: file multiply.c, line 18. (rr) reverse-continue Continuing. Breakpoint 3, multiply (a=0x7ffe0bea5b58 "", size=4, mult=0) at multiply.c:18 18 for (i=0; i0) (rr) c Continuing. &lt;/pre&gt; &lt;h3&gt;Wait!&lt;/h3&gt; &lt;p&gt;What happened to the &lt;code&gt;mult&lt;/code&gt; argument? It should be 2. Zero times anything is zero. That explains the results. However, how did the &lt;code&gt;main&lt;/code&gt; function&amp;#8217;s local variable &lt;code&gt;mult&lt;/code&gt; end up being zero? It is initialized in &lt;code&gt;main&lt;/code&gt; and only passed to the compute function. Let&amp;#8217;s set a hardware watchpoint on &lt;code&gt;mult&lt;/code&gt; and continue the reverse execution of the program:&lt;/p&gt; &lt;pre&gt;(rr) up #1 0x0000000000401247 in main (argc=1, argv=0x7ffe0bea5c58) at multiply.c:35 35 multiply(a, SIZE, mult); (rr) watch -l mult Hardware watchpoint 4: -location mult (rr) reverse-continue Continuing. Hardware watchpoint 4: -location mult Old value = 0 New value = 2 zero (a=0x7ffe0bea5b58 "", size=3) at multiply.c:7 7 a[size--] = 0; &lt;/pre&gt; &lt;p&gt;Ah, now it&amp;#8217;s becoming clear how things went wrong. The &lt;code&gt;zero&lt;/code&gt; function wrote past the end of the &lt;code&gt;a&lt;/code&gt; array and overwrote the &lt;code&gt;mult&lt;/code&gt; variable even though it wasn&amp;#8217;t passed in. The &lt;code&gt;size--&lt;/code&gt; statement should be &lt;code&gt;--size&lt;/code&gt;. We can see the call to &lt;code&gt;zero&lt;/code&gt; hidden in the &lt;code&gt;initialize&lt;/code&gt; function:&lt;/p&gt; &lt;pre&gt;(rr) where #0 zero (a=0x7ffe0bea5b58 "", size=3) at multiply.c:7 #1 0x0000000000401173 in initialize (a=0x7ffe0bea5b58 "", size=4) at multiply.c:12 #2 0x0000000000401233 in main (argc=1, argv=0x7ffe0bea5c58) at multiply.c:34 &lt;/pre&gt; &lt;p&gt;If we want, we can use regular GDB continues and play it forward and go through those breakpoints we set earlier again:&lt;/p&gt; &lt;pre&gt;(rr) c Continuing. Hardware watchpoint 4: -location mult Old value = 2 New value = 0 zero (a=0x7ffe0bea5b58 "", size=3) at multiply.c:6 6 while (size&amp;#62;0) (rr) c Continuing. Breakpoint 3, multiply (a=0x7ffe0bea5b58 "", size=4, mult=0) at multiply.c:18 18 for (i=0; i&amp;#60;size; i++) (rr) c Continuing. Breakpoint 2, pr_array (a=0x7ffe0bea5b58 "", size=4) at multiply.c:25 25 for (i=0; i&amp;#60;size; i++) &lt;/pre&gt; &lt;h3&gt;Fixing the issue&lt;/h3&gt; &lt;p&gt;Now that we&amp;#8217;ve identified the problem, we can fix the &lt;code&gt;zero&lt;/code&gt; function in the &amp;#8220;new and improved&amp;#8221; &lt;code&gt;multiply2.c&lt;/code&gt; program, and things work as expected:&lt;/p&gt; &lt;pre&gt;$ gcc -g multiply2.c -o multiply2 $ ./multiply2 f(0)=0 f(1)=2 f(2)=4 f(3)=6 &lt;/pre&gt; &lt;h2&gt;Limitations of rr&lt;/h2&gt; &lt;p&gt;Although &lt;code&gt;rr&lt;/code&gt; is a useful addition to the tools that help developers find issues in programs, it does have limitations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;All the threads that &lt;code&gt;rr&lt;/code&gt; records run on a single core, so multithreaded applications will be slower.&lt;/li&gt; &lt;li&gt;&lt;code&gt;rr&lt;/code&gt; syscall monitoring is not complete, so some syscalls might slip through the cracks and not be recorded in the trace.&lt;/li&gt; &lt;li&gt;&lt;code&gt;rr&lt;/code&gt; uses &lt;code&gt;ptrace&lt;/code&gt; to monitor apps and will not work well with apps that also use &lt;code&gt;ptrace&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;code&gt;rr&lt;/code&gt; does not monitor processes outside the children of what it is recording, and misses any communication through shared memory to an outside process.&lt;/li&gt; &lt;li&gt;&lt;code&gt;rr&lt;/code&gt; operates only on very specific processor microarchitectures.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;The ability of &lt;code&gt;rr&lt;/code&gt; to go backward in the execution of a program to investigate the earlier events that lead to a problem is a useful addition to the developer&amp;#8217;s set of tools. The example in this article illustrates how you might use &lt;code&gt;rr&lt;/code&gt; to track down problems. The example was just a toy, but &lt;code&gt;rr&lt;/code&gt; has been used to track down problems in much much more substantial applications such as &lt;a target="_blank" rel="nofollow" href="https://blog.ret2.io/2018/06/19/pwn2own-2018-root-cause-analysis/"&gt;JavaScriptCore&lt;/a&gt;, the &lt;a target="_blank" rel="nofollow" href="https://animal0day.blogspot.com/2017/07/from-fuzzing-apache-httpd-server-to-cve.html"&gt;Apache httpd server&lt;/a&gt;, and &lt;a target="_blank" rel="nofollow" href="https://www.freshworks.com/saas/debugging-memory-corruption-in-production-rails-app-using-mozilla-rr-blog/"&gt;Ruby on Rails&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;See the following resources for additional information:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://rr-project.org/"&gt;rr-project page&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://arxiv.org/pdf/1705.05937.pdf"&gt;Engineering Record And Replay For Deployability Extended Technical Report&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Finstant-replay-debugging-c-and-c-programs-with-rr%2F&amp;#38;linkname=Instant%20replay%3A%20Debugging%20C%20and%20C%2B%2B%20programs%20with%20rr" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Finstant-replay-debugging-c-and-c-programs-with-rr%2F&amp;#38;linkname=Instant%20replay%3A%20Debugging%20C%20and%20C%2B%2B%20programs%20with%20rr" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Finstant-replay-debugging-c-and-c-programs-with-rr%2F&amp;#38;linkname=Instant%20replay%3A%20Debugging%20C%20and%20C%2B%2B%20programs%20with%20rr" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Finstant-replay-debugging-c-and-c-programs-with-rr%2F&amp;#38;linkname=Instant%20replay%3A%20Debugging%20C%20and%20C%2B%2B%20programs%20with%20rr" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Finstant-replay-debugging-c-and-c-programs-with-rr%2F&amp;#38;linkname=Instant%20replay%3A%20Debugging%20C%20and%20C%2B%2B%20programs%20with%20rr" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Finstant-replay-debugging-c-and-c-programs-with-rr%2F&amp;#38;linkname=Instant%20replay%3A%20Debugging%20C%20and%20C%2B%2B%20programs%20with%20rr" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Finstant-replay-debugging-c-and-c-programs-with-rr%2F&amp;#38;linkname=Instant%20replay%3A%20Debugging%20C%20and%20C%2B%2B%20programs%20with%20rr" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Finstant-replay-debugging-c-and-c-programs-with-rr%2F&amp;#038;title=Instant%20replay%3A%20Debugging%20C%20and%20C%2B%2B%20programs%20with%20rr" data-a2a-url="https://developers.redhat.com/blog/2021/05/03/instant-replay-debugging-c-and-c-programs-with-rr/" data-a2a-title="Instant replay: Debugging C and C++ programs with rr"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/05/03/instant-replay-debugging-c-and-c-programs-with-rr/"&gt;Instant replay: Debugging C and C++ programs with rr&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/k-l5ALdtZH4" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;The common theme in many time-travel movies is to go back in time to find out what went wrong and fix it. Developers also have that desire to go back in time and find why the code broke and fix it. But, often, that crucial step where everything went wrong happened long ago, and the [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/05/03/instant-replay-debugging-c-and-c-programs-with-rr/"&gt;Instant replay: Debugging C and C++ programs with rr&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/05/03/instant-replay-debugging-c-and-c-programs-with-rr/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">862007</post-id><dc:creator>William Cohen</dc:creator><dc:date>2021-05-03T07:00:45Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/05/03/instant-replay-debugging-c-and-c-programs-with-rr/</feedburner:origLink></entry><entry><title>New features in OpenMP 5.0 and 5.1</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/tH2Zp36b-pw/" /><category term="C" /><category term="Developer Tools" /><category term="Linux" /><category term="Performance" /><category term="c++11" /><category term="GCC 11" /><category term="OpenMP" /><author><name>Jakub Jelínek</name></author><id>https://developers.redhat.com/blog/?p=801277</id><updated>2021-05-03T07:00:22Z</updated><published>2021-05-03T07:00:22Z</published><content type="html">&lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://www.openmp.org"&gt;OpenMP&lt;/a&gt; is an API consisting of compiler directives and library routines for high-level parallelism in &lt;a target="_blank" rel="nofollow" href="/topics/c"&gt;C and C++&lt;/a&gt;, as well as &lt;a target="_blank" rel="nofollow" href="https://opensource.com/article/17/11/happy-60th-birthday-fortran"&gt;Fortran&lt;/a&gt;. &lt;a target="_blank" rel="nofollow" href="https://www.openmp.org/spec-html/5.1/openmp.html"&gt;Version 5.1&lt;/a&gt; of OpenMP was released in November 2020 and &lt;a target="_blank" rel="nofollow" href="https://www.openmp.org/spec-html/5.0/openmp.html"&gt;version 5.0&lt;/a&gt; was released in November 2018. This article discusses the new features from OpenMP 5.0 which are implemented in GCC 11, and some new OpenMP 5.1 features.&lt;/p&gt; &lt;h2&gt;OpenMP 5.0 features&lt;/h2&gt; &lt;p&gt;Let&amp;#8217;s start with features that were added in the OpenMP 5.0 standard version.&lt;/p&gt; &lt;h3&gt;Support for non-rectangular collapsed loops&lt;/h3&gt; &lt;p&gt;Before OpenMP 5.0, all OpenMP looping constructs (worksharing loops, &lt;code&gt;simd&lt;/code&gt;, &lt;code&gt;distribute&lt;/code&gt;, &lt;code&gt;taskloop&lt;/code&gt;, and combined or composite constructs based on those) were required to be rectangular. This means that all of the lower bound, upper bound, and increment expressions of all the associated loops in the loop nest were required to be invariant against the outermost loop. OpenMP 5.0 still requires all the increment expressions to be loop-invariant, but allows some cases where the lower and upper bound expressions of the inner loops can be based on a single outer-loop iterator.&lt;/p&gt; &lt;p&gt;There are restrictions to this new feature, however: An inner-loop iterator must use at most one outer-loop iterator, and the expressions need to resolve to &lt;em&gt;a * outer + b&lt;/em&gt;, where &lt;em&gt;a&lt;/em&gt; and &lt;em&gt;b&lt;/em&gt; are loop-invariant expressions. If the inner and referenced outer loops have different increments, there are further restrictions to support easy computation of the number of iterations of the collapsed loop nest before the loop. In addition, non-rectangular loops might not have &lt;code&gt;schedule&lt;/code&gt; or &lt;code&gt;dist_schedule&lt;/code&gt; clauses specified. This allows the implementation to choose any iteration distribution it prefers.&lt;/p&gt; &lt;p&gt;The following triangular loop is an example:&lt;/p&gt; &lt;pre&gt;#pragma omp for collapse(2) for (int i = 0; i &amp;#60; 100; i++) for (int j = 0; j &amp;#60; i; j++) arr[i][j] = compute (i, j);&lt;/pre&gt; &lt;p&gt;But a non-rectangular loop can also be much more complex:&lt;/p&gt; &lt;pre&gt;#pragma omp distribute parallel for simd collapse(4) for (int i = 0; i &amp;#60; 20; i++) for (int j = a; j &amp;#62;= g + i * h; j -= n) for (int k = 0; k &amp;#60; i; k++) for (int l = o * j; l &amp;#60; p; l += q) arr[i][j][k][l] = compute (i, j, k, l);&lt;/pre&gt; &lt;p&gt;The easiest implementation is by computing a rectangular hull of the loop nest and doing nothing inside of the combined loop body for iterations that wouldn&amp;#8217;t be run by the original loop. For example, for the first loop in this section, the implementation would be:&lt;/p&gt; &lt;pre&gt;#pragma omp for collapse(2) for (int i = 0; i &amp;#60; 100; i++) for (int j = 0; j &amp;#60; 100; j++) if (j &amp;#60; i) arr[i][j] = compute (i, j);&lt;/pre&gt; &lt;p&gt;Unfortunately, such an implementation can cause a significant work imbalance where some threads do no real work at all. Therefore, except for non-combined non-rectangular &lt;code&gt;simd&lt;/code&gt; constructs, GCC 11 computes an accurate number of iterations before the loop. In the case of loop nests with just one loop dependent on outer-loop iterators, it uses Faulhaber&amp;#8217;s formula, with adjustments for the fact that some values of the outer iterator might result in no iterations of the inner loop. This way, as long as the loop body performs roughly the same amount of work for each iteration, the work is distributed evenly.&lt;/p&gt; &lt;h3&gt;Conditional lastprivate&lt;/h3&gt; &lt;p&gt;In OpenMP, the &lt;code&gt;lastprivate&lt;/code&gt; clause can be used to retrieve the value of the privatized variable that was assigned in the last iteration of the loop. The &lt;code&gt;lastprivate&lt;/code&gt; clause with a conditional modifier works as a fancy reduction, which chooses the value from the thread (or team, SIMD lane, or task) that executed the maximum logical iteration number. For example:&lt;/p&gt; &lt;pre&gt;#pragma omp parallel for lastprivate(conditional:v) for (int i = 0; i &amp;#60; 1024; i++) if (cond (i)) v = compute (i); result (v);&lt;/pre&gt; &lt;p&gt;For this construct to work, the privatized variable must be modified only by storing directly to it, and shouldn&amp;#8217;t be modified through pointers or modified inside of other functions. This allows the implementation to find those stores easily and adjust a store to remember the logical iteration that stored it. This feature is implemented in GCC 10 already.&lt;/p&gt; &lt;h3&gt;Inclusive and exclusive scan support&lt;/h3&gt; &lt;p&gt;OpenMP 5.0 added support for implementing parallel prefix sums (otherwise known as cumulative sums or inclusive and exclusive scans). This support allows C++17 &lt;code&gt;std::inclusive_scan&lt;/code&gt; and &lt;code&gt;std::exclusive_scan&lt;/code&gt; to be parallelized using OpenMP. The syntax is built upon the &lt;code&gt;reduction&lt;/code&gt; clause with a special modifier and a new directive that divides the loop body into two halves. For example:&lt;/p&gt; &lt;pre&gt;#pragma omp parallel for reduction (inscan, +:r) for (int i = 0; i &amp;#60; 1024; i++) { r += a[i]; #pragma omp scan inclusive(r) b[i] = r; }&lt;/pre&gt; &lt;p&gt;The implementation can then split the loop into the two halves, creating not just one privatized variable per thread, but a full array for the entire construct. After evaluating one of the halves of user code for all iterations—which differs between inclusive and exclusive scans—efficient parallel computation of the prefix sum can be performed on the privatized array, and finally, the other half of the user code can be evaluated by all threads. The syntax allows the code to work properly even when the OpenMP pragmas are ignored. This feature is implemented in GCC 10.&lt;/p&gt; &lt;h3&gt;Declare variant support and meta-directives&lt;/h3&gt; &lt;p&gt;In OpenMP 5.0, some direct calls can be redirected to specialized alternative implementations based on the OpenMP context from which they are called. The specialization can be done based on which OpenMP constructs the call site is lexically nested in. The OpenMP implementation can then select the correct alternative based upon the implementation vendor, the CPU architecture and ISA flags for which the code is compiled, and so on. Here is an example:&lt;/p&gt; &lt;pre&gt;void foo_parallel_for (void); void foo_avx512 (void); void foo_ptx (void); #pragma omp declare variant (foo_parallel_for) \ match (construct={parallel,for},device={kind("any")}) #pragma omp declare variant (foo_avx512) \ match (device={isa(avx512bw,avx512vl,"avx512f")}) #pragma omp declare variant (foo_ptx) match (device={arch("nvptx")}) void foo (void);&lt;/pre&gt; &lt;p&gt;If &lt;code&gt;foo&lt;/code&gt; is called directly from within the lexical body of a worksharing loop that is lexically nested in a parallel construct (including the combined &lt;code&gt;parallel for&lt;/code&gt;), the call will be replaced by a call to &lt;code&gt;foo_parallel_for&lt;/code&gt;. If &lt;code&gt;foo&lt;/code&gt; is called from code compiled for the previously mentioned AVX512 ISAs, &lt;code&gt;foo_avx512&lt;/code&gt; will be called instead. And finally, if &lt;code&gt;foo&lt;/code&gt; is called from code running on NVidia PTX, the compiler will call &lt;code&gt;foo_ptx&lt;/code&gt; instead.&lt;/p&gt; &lt;p&gt;A complex scoring system, including user scores, decides which variant will be used in case multiple variants match. This construct is partially supported in GCC 10 and fully supported in GCC 11. The OpenMP 5.0 specification also allows meta-directives using similar syntax, where one of several different OpenMP directives can be used depending on the OpenMP context in which it is used.&lt;/p&gt; &lt;h3&gt;The loop construct&lt;/h3&gt; &lt;p&gt;In OpenMP 4.5, the various looping constructs prescribed to the implementation how it should divide the work. A programmer specified whether the work should be divided between teams in the league of teams, or between threads in the parallel region, or across SIMD lanes in a &lt;code&gt;simd&lt;/code&gt; construct, and so on. OpenMP 5.0 offers a new loop construct that is less prescriptive and leaves more freedom to the implementation about how to actually implement the work division. Here&amp;#8217;s an example:&lt;/p&gt; &lt;pre&gt;#pragma omp loop bind(thread) collapse(2) for (int i = 0; i &amp;#60; 1024; i++) for (int j = 0; j &amp;#60; 1024; j++) a[i][j] = work (i, j);&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;bind&lt;/code&gt; clause is required on orphaned constructs and specifies which kind of threads that encounter it will participate in the construct. If the pragma is lexically nested in an OpenMP construct that makes the binding obvious, the bind clause can be omitted. The implementation is allowed to use extra threads to execute the iterations. The loop construct is implemented in GCC 10.&lt;/p&gt; &lt;p&gt;There are restrictions on which OpenMP directives can appear in the body of the loop, and no OpenMP API calls can be used there. These restrictions were imposed so that the user program can&amp;#8217;t observe and rely on how the directive is actually implemented. Restrictions on work scheduling have been added in OpenMP 5.1, which is discussed next.&lt;/p&gt; &lt;h2&gt;OpenMP 5.1 features&lt;/h2&gt; &lt;p&gt;In OpenMP 5.1, C++ programs can specify OpenMP directives using C++11 attributes, in addition to the older use of pragmas. Two examples using attributes follow:&lt;/p&gt; &lt;pre&gt;[[omp::directive (parallel for, schedule(static))]] for (int i = 0; i &amp;#60; 1024; i++) a[i] = work (I); [[omp::sequence (directive (parallel, num_threads(16)), \ directive (for, schedule(static, 32)))]] for (int i = 0; i &amp;#60; 1024; i++) a[i] = work (i);&lt;/pre&gt; &lt;p&gt;OpenMP 5.1 added a scope directive, where all threads encountering it will execute the body of the construct. Private and reduction clauses can be applied to it. For example:&lt;/p&gt; &lt;pre&gt;#pragma omp scope private (i) reduction(+:r) { i = foo (); r += i; }&lt;/pre&gt; &lt;p&gt;Unless the &lt;code&gt;nowait&lt;/code&gt; clause is present on the directive, there is an implicit barrier at the end of the region.&lt;/p&gt; &lt;p&gt;OpenMP 5.1 has new &lt;code&gt;assume&lt;/code&gt;, &lt;code&gt;interop&lt;/code&gt;, &lt;code&gt;dispatch&lt;/code&gt;, &lt;code&gt;error&lt;/code&gt;, and &lt;code&gt;nothing&lt;/code&gt; directives. Loop transformation directives were also added. The master was deprecated and replaced by the new masked construct. There are many new API calls, including:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;omp_target_is_accessible&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;omp_get_mapped_ptr&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;omp_calloc&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;omp_aligned_alloc&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;omp_realloc&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;omp_set_num_teams&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;omp_set_teams_thread_limit&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;omp_get_max_teams&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;omp_get_teams_thread_limit&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The &lt;a target="_blank" rel="nofollow" href="https://www.openmp.org/spec-html/5.1/openmpap2.html#x349-524000B"&gt;OpenMP API features history appendix&lt;/a&gt; covers all changes, including deprecated features.&lt;/p&gt; &lt;h2&gt;Try it out&lt;/h2&gt; &lt;p&gt;The specifications for both OpenMP 5.0 and OpenMP 5.1 are available at &lt;a target="_blank" rel="nofollow" href="https://www.openmp.org/specifications/"&gt;openmp.org/specifications/&lt;/a&gt;, including both PDF and HTML layouts. The latest version of GCC (&lt;a target="_blank" rel="nofollow" href="https://gcc.gnu.org/gcc-11/changes.html"&gt;GCC 11&lt;/a&gt;) supports the features described in this article and various others (this time not just C and C++, but many features also for Fortran). But several other new features of OpenMP will be implemented only in later GCC versions.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Fnew-features-in-openmp-5-0-and-5-1%2F&amp;#38;linkname=New%20features%20in%20OpenMP%205.0%20and%205.1" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Fnew-features-in-openmp-5-0-and-5-1%2F&amp;#38;linkname=New%20features%20in%20OpenMP%205.0%20and%205.1" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Fnew-features-in-openmp-5-0-and-5-1%2F&amp;#38;linkname=New%20features%20in%20OpenMP%205.0%20and%205.1" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Fnew-features-in-openmp-5-0-and-5-1%2F&amp;#38;linkname=New%20features%20in%20OpenMP%205.0%20and%205.1" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Fnew-features-in-openmp-5-0-and-5-1%2F&amp;#38;linkname=New%20features%20in%20OpenMP%205.0%20and%205.1" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Fnew-features-in-openmp-5-0-and-5-1%2F&amp;#38;linkname=New%20features%20in%20OpenMP%205.0%20and%205.1" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Fnew-features-in-openmp-5-0-and-5-1%2F&amp;#38;linkname=New%20features%20in%20OpenMP%205.0%20and%205.1" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Fnew-features-in-openmp-5-0-and-5-1%2F&amp;#038;title=New%20features%20in%20OpenMP%205.0%20and%205.1" data-a2a-url="https://developers.redhat.com/blog/2021/05/03/new-features-in-openmp-5-0-and-5-1/" data-a2a-title="New features in OpenMP 5.0 and 5.1"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/05/03/new-features-in-openmp-5-0-and-5-1/"&gt;New features in OpenMP 5.0 and 5.1&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/tH2Zp36b-pw" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;OpenMP is an API consisting of compiler directives and library routines for high-level parallelism in C and C++, as well as Fortran. Version 5.1 of OpenMP was released in November 2020 and version 5.0 was released in November 2018. This article discusses the new features from OpenMP 5.0 which are implemented in GCC 11, and [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/05/03/new-features-in-openmp-5-0-and-5-1/"&gt;New features in OpenMP 5.0 and 5.1&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/05/03/new-features-in-openmp-5-0-and-5-1/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">801277</post-id><dc:creator>Jakub Jelínek</dc:creator><dc:date>2021-05-03T07:00:22Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/05/03/new-features-in-openmp-5-0-and-5-1/</feedburner:origLink></entry><entry><title>Red Hat Software Collections 3.7 and Red Hat Developer Toolset 10.1 beta versions now available</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/XPToxqimAyE/" /><category term="C" /><category term="Developer Tools" /><category term="Java" /><category term="Linux" /><category term="Performance" /><category term="developer toolset" /><category term="gcc" /><category term="RHEL" /><category term="software collections" /><author><name>Brian Gollaher</name></author><id>https://developers.redhat.com/blog/?p=901657</id><updated>2021-05-03T04:03:18Z</updated><published>2021-05-03T04:03:18Z</published><content type="html">&lt;p&gt;The latest versions of &lt;a target="_blank" rel="nofollow" href="/products/softwarecollections/overview"&gt;Red Hat Software Collections&lt;/a&gt; and &lt;a target="_blank" rel="nofollow" href="/products/developertoolset/overview"&gt;Red Hat Developer Toolset&lt;/a&gt; are available now in beta. Software Collections 3.7 delivers the latest stable versions of many popular open source runtime languages, web servers, and databases natively to the &lt;a target="_blank" rel="nofollow" href="/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; platform. These components are supported for up to five years, supporting a more consistent, efficient, and reliable developer experience.&lt;/p&gt; &lt;h2&gt;What&amp;#8217;s new in Red Hat Software Collections 3.7&lt;/h2&gt; &lt;p&gt;New and updated collections in the latest release of Red Hat Software Collections include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;PostgreSQL 13&lt;/strong&gt;: This version provides many new features and enhancements not found in version 12. Notable changes include performance improvements resulting from deduplication of B-tree index entries, improved performance for queries that use aggregates or partitioned tables, improved query planning when using extended statistics, parallelized vacuuming of indexes, and incremental sorting.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MariaDB 10.5&lt;/strong&gt;: Notable enhancements over the previously available version, 10.3, include a number of security updates, new features, and updates to the InnoDB storage engine. In addition, the MariaDB Galera Cluster has been upgraded to version 4.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ruby&lt;/strong&gt; 3.0: This version provides a number of bug fixes and enhancements not found in Ruby 2.7. This new major release brings speed improvements, introduces language to describe the types (RBS), and provides concurrent abstraction via Ractor. The new version now also separates keyword arguments from other arguments.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Java Mission Control 8.0.0 (update)&lt;/strong&gt;: This is an advanced set of tools for managing, monitoring, profiling, and troubleshooting &lt;a target="_blank" rel="nofollow" href="/topics/enterprise-java"&gt;Java&lt;/a&gt; applications. Updates to Java Mission Control include new graphs and viewers for stack traces, threads, and memory usage.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Also new in Software Collections 3.7 is Developer Toolset 10.1, which features GNU Compiler Collection (GCC) 10.2.1, a new update of the popular free software compiler collection. GCC is a curated collection of compilers, toolchains, debuggers, and other critical development tools. Additional updates in Developer Toolset 10.1 center on delivering new updates of &lt;a target="_blank" rel="nofollow" href="/topics/c"&gt;C/C++&lt;/a&gt; and Fortran debugging and &lt;a target="_blank" rel="nofollow" href="/blog/category/performance/"&gt;performance tools&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;All new collections in Software Collections 3.7 are also available as &lt;a target="_blank" rel="nofollow" href="https://connect.redhat.com/explore/red-hat-container-certification"&gt;Red Hat Certified Containers&lt;/a&gt; through the &lt;a target="_blank" rel="nofollow" href="https://catalog.redhat.com/software/containers/explore"&gt;Red Hat Ecosystem Catalog&lt;/a&gt;. This makes it easier to build and deploy applications using the supported components of Software Collections for Red Hat Enterprise Linux and &lt;a target="_blank" rel="nofollow" href="/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; environments.&lt;/p&gt; &lt;p&gt;Software Collections 3.7 continues Red Hat’s commitment to customer choice in underlying compute architecture, with availability across x86_64, ppc64, ppc64le, and s390x hardware.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Red Hat customers with active &lt;a href="https://developers.redhat.com/blog/2019/08/21/why-you-should-be-developing-on-red-hat-enterprise-linux/"&gt;Red Hat Enterprise Linux&lt;/a&gt; subscriptions can access Software Collections via the &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/solutions/472793"&gt;Red Hat Software Collections repository&lt;/a&gt;. For more information, please read the full &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_software_collections/3-beta/"&gt;beta release notes&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Fred-hat-software-collections-3-7-and-red-hat-developer-toolset-10-1-beta-versions-now-available%2F&amp;#38;linkname=Red%20Hat%20Software%20Collections%203.7%20and%20Red%20Hat%20Developer%20Toolset%2010.1%20beta%20versions%20now%20available" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Fred-hat-software-collections-3-7-and-red-hat-developer-toolset-10-1-beta-versions-now-available%2F&amp;#38;linkname=Red%20Hat%20Software%20Collections%203.7%20and%20Red%20Hat%20Developer%20Toolset%2010.1%20beta%20versions%20now%20available" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Fred-hat-software-collections-3-7-and-red-hat-developer-toolset-10-1-beta-versions-now-available%2F&amp;#38;linkname=Red%20Hat%20Software%20Collections%203.7%20and%20Red%20Hat%20Developer%20Toolset%2010.1%20beta%20versions%20now%20available" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Fred-hat-software-collections-3-7-and-red-hat-developer-toolset-10-1-beta-versions-now-available%2F&amp;#38;linkname=Red%20Hat%20Software%20Collections%203.7%20and%20Red%20Hat%20Developer%20Toolset%2010.1%20beta%20versions%20now%20available" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Fred-hat-software-collections-3-7-and-red-hat-developer-toolset-10-1-beta-versions-now-available%2F&amp;#38;linkname=Red%20Hat%20Software%20Collections%203.7%20and%20Red%20Hat%20Developer%20Toolset%2010.1%20beta%20versions%20now%20available" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Fred-hat-software-collections-3-7-and-red-hat-developer-toolset-10-1-beta-versions-now-available%2F&amp;#38;linkname=Red%20Hat%20Software%20Collections%203.7%20and%20Red%20Hat%20Developer%20Toolset%2010.1%20beta%20versions%20now%20available" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Fred-hat-software-collections-3-7-and-red-hat-developer-toolset-10-1-beta-versions-now-available%2F&amp;#38;linkname=Red%20Hat%20Software%20Collections%203.7%20and%20Red%20Hat%20Developer%20Toolset%2010.1%20beta%20versions%20now%20available" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Fred-hat-software-collections-3-7-and-red-hat-developer-toolset-10-1-beta-versions-now-available%2F&amp;#038;title=Red%20Hat%20Software%20Collections%203.7%20and%20Red%20Hat%20Developer%20Toolset%2010.1%20beta%20versions%20now%20available" data-a2a-url="https://developers.redhat.com/blog/2021/05/03/red-hat-software-collections-3-7-and-red-hat-developer-toolset-10-1-beta-versions-now-available/" data-a2a-title="Red Hat Software Collections 3.7 and Red Hat Developer Toolset 10.1 beta versions now available"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/05/03/red-hat-software-collections-3-7-and-red-hat-developer-toolset-10-1-beta-versions-now-available/"&gt;Red Hat Software Collections 3.7 and Red Hat Developer Toolset 10.1 beta versions now available&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/XPToxqimAyE" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;The latest versions of Red Hat Software Collections and Red Hat Developer Toolset are available now in beta. Software Collections 3.7 delivers the latest stable versions of many popular open source runtime languages, web servers, and databases natively to the Red Hat Enterprise Linux platform. These components are supported for up to five years, supporting [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/05/03/red-hat-software-collections-3-7-and-red-hat-developer-toolset-10-1-beta-versions-now-available/"&gt;Red Hat Software Collections 3.7 and Red Hat Developer Toolset 10.1 beta versions now available&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/05/03/red-hat-software-collections-3-7-and-red-hat-developer-toolset-10-1-beta-versions-now-available/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">901657</post-id><dc:creator>Brian Gollaher</dc:creator><dc:date>2021-05-03T04:03:18Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/05/03/red-hat-software-collections-3-7-and-red-hat-developer-toolset-10-1-beta-versions-now-available/</feedburner:origLink></entry><entry><title type="html">Versioning of utility classes in RHPAM</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/9nQPAkGSBYI/versioning-of-utility-classes-in-rhpam.html" /><author><name>Diego Torres Fuerte</name></author><id>https://blog.kie.org/2021/05/versioning-of-utility-classes-in-rhpam.html</id><updated>2021-05-02T19:20:40Z</updated><content type="html">In this post, we will describe how to perform versioning of utility classes in RHPAM. After running your first process instances in a fresh RHPAM installation, the next time you will deploy something to the environment, is when your processes and business assets have evolved. Your new business assets will require to integrate their changes with the current client applications, as well as with the running process instances. Note that the parts of the code that can possibly evolve, can be classified as: * Utility Classes * Model classes * Rules * Process definitions. Firstly, let’s define Utility Classes. We often use utility classes in the script tasks and events, mainly for mapping process variables to service models. Thus Utility Classes define common functionality. When using Utility Classes, keep in mind the following list of best practices: * Define these methods to be static. * Avoid System.out logs. * Do not use thread operations like sleep or start thread. * Keep away from calling remote services in these utility classes. THE USE CASE: CORRECTIVE REPAIRS TO MANUFACTURING MACHINERY. Let’s consider an example, where multiple sensors in a manufacturing factory, report the state of the machine’s health, and other performance indicators. These sensors trigger Decision Management Services, that consequently will start a process for parts procurement: The image above shows the “parts procurement process”. In short, the conversation models for web services are calculated with the help of PartsStorageUtil and PurchaseOrderUtil classes. RUNNING THE UTILITY CLASS IN ITS OWN VERSION The growth of the organization requires a change in the InventoryReservationRequest. When in the new version, all requests for inventory, will go through a central, and the parts requests will be dispatched to different branches. In order to program the delivery of the parts, you are required to provide the Branch Code. In conclusion, the requirement you get from the business is: While my branches will be submitting new purchase orders to the central, any request that was previously originated in the original branch can continue to be processed in that branch. Similarly, this means for my process service that: Any existing process instance running in the version 1.0 of my procurement process instance, can continue to run in its version; likewise, any new procurement order that includes the branch code for the central, will use the new version of the utility class. The idea then for example, is that: 1. You start a process instance in version 1.0. 2. There are no parts available to fulfill the materials request. 3. The Process instance reaches the Purchase Order, and it is waiting for the “Received Materials” signal. At this point, we introduce the change for the “Request Assign parts to Repair Request”. Thus, any new requests will need to include the “Branch Code” field. As a result, the existing process instance, will continue to send the “Assign Parts to Repair Request” without “Branch Code”. UTILITY CLASS AS A DEPENDENCY The utility classes can be packaged in a jar file. Consequently, the jar file can be added as a dependency of the kjar where we want to use the utility classes. This allows changing the utility classes in a single place, and then, promote its use throughout the kjars. By looking to the following image, you can notice that, in my example, I define the parts-storage-service and the purchase-orders-service web services. Notice that, in both web services, I define a domain model that dictates how its client applications can interact with them. In the image above, you can see the relationship between the kjar (machinery-repair), the utility dependencies (jbpm-purchase-order-utils, and jbpm-parts-storage-utils), and the model jars that define the interaction with the web services. STEPS FOR THE INITIAL VERSION The initial version is created using the following steps: 1. Download and build the domain model jars from:  1.   2.   2. Download and run the web services from: 1. 2.   3. Download and build the test helper library: 4. Download and build the utility libraries: 1. 2.   5. Download and build the kjar project: 6. Deploy the kjar project to a kie-server that has persistence configuration to a relational database. 7. Start 3 process instances for the machinery-repair.parts-procurement_v1_0 process id. Given that the inventory service does not have any parts, all process instances will request a purchase order, and wait for the “Received Materials” signal. STEPS TO PRODUCE THE CHANGE The following steps create the version 2 of the kjar that connects to the web services version 2, that require the “Branch Code”: 1. Change the domain models to include the Branch Code field. (Inventory Reservation Request Model). 2. Modify the web service to use the branch code when requesting and responding to an inventory reservation. 3. Adapt the utility classes to read a process variable for the Branch Code and include it as part of the payload for the web service requests. 4. Adjust the process definition to receive the “Branch Code” as a process variable. 5. Deploy the kjar with new version (suggested as 2.0) to the kie-server. (The kie-server will have 2 containers now, the 1.0, and the 2.0). 6. Start 3 process instances for the machinery-repair.parts-procurement_v1_0 process id in the latest version. 7. Signal the previously existing process instances For each change, remember to: * Increase the component version in the pom.xml. * Refactor and run the appropriate unit tests. Note that the new process instances, and the old process instances use their own versions of the utility class. This helps to provide an example on versioning of utility classes RHPAM, by packaging the utility classes as dependencies for the RHPAM kjar. INSTALL THE MODIFIED ASSETS In the following gitHub links you can find the solution to the previous exercise, install accordingly and compare with your results: 1. Download and build the domain model jar, the web service, the utility library and the kjar project from: 1. 2.   3. 4.   2. Deploy the kjar project to the same kie-server, using the same alias used in the previous section for version 1.0.0; 3. Start 3 additional process instances for machinery-repair.parts-procurement_v2_0 process id. 4. Send the signal to the first 3 process instances, and note how they continue to interact with the services version 1.0 with no interruption. 5. Send the signal to the latest 3 process instances, and note how they continue to interact with the services version 2.0 with no interruption. CONCLUSION By setting the dependencies and separating the utility classes in their own jars, the process definitions are capable of executing their own version of the utility class in the same kie-server, thus providing an option for versioning of utility classes in RHPAM. The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/9nQPAkGSBYI" height="1" width="1" alt=""/&gt;</content><dc:creator>Diego Torres Fuerte</dc:creator><feedburner:origLink>https://blog.kie.org/2021/05/versioning-of-utility-classes-in-rhpam.html</feedburner:origLink></entry><entry><title type="html">Building Dashboards using Plain Java</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/A_K5iDugtWo/building-dashboards-using-plain-java.html" /><author><name>William Siqueira</name></author><id>https://blog.kie.org/2021/04/building-dashboards-using-plain-java.html</id><updated>2021-04-30T18:44:57Z</updated><content type="html">is a great tool to author dashboards that can consume and display data from many sources, i.e. Prometheus, JDBC, Kie Server, and others. These dashboards can be later exported to a ZIP file and then be executed on . Dashboard built with DashBuilder Recently, we added a new alternative to create those dashboards: a pure Java API. The Java API exposes our existing builders and factories for data sets and visual components, but it also brings a new set of components for pages and navigation. View the code on . View the code on . In this post, we will introduce the DashBuilder Java DSL (Domain Specific Language) API and teach you how to get started with it. DASHBUIDER DSL API The API has 5 main core components. Let’s take a look at those: DATA SETS The API for data sets has as entry point the class org.dashbuilder.dataset.def.DataSetDefFactory which allows you to create your own data set definition. You can also pure Java data set using the class DataSetFactory, for example: View the code on . It is also possible to build data sets from multiple sources, such as CSV, Prometheus, SQL, and many others. It is important to notice that the same data set can have a different representation using lookups, which can be understood as data sets transformation requests. We will explore this later. COMPONENTS Components can be used in a dashboard page to display a data set or static information. The entry point to create components is the class org.dashbuilder.dsl.factory.component.ComponentFactory. From there you can refer to components in your local file system or build displayers components. For “displayer” components notice that we need a factory to build the settings, the factory is class: org.dashbuilder.displayer.DisplayerSettingsFactory, where you can build the settings for your displayer. Bear in mind that all classes that use a displayer will require a data set to be built, otherwise, an exception will be thrown when exporting the dashboard; PAGE You can build pages composed of the components mentioned in 2. Pages have rows that can have columns and finally components. Using the class org.dashbuilder.dsl.factory.page.PageFactory allow make it easy for you to create any page component; NAVIGATION You can create the pages navigation using org.dashbuilder.dsl.factory.navigation.NavigationFactory. This class can be used to define the menu of pages that will be displayed in DashBuilder Runtime; DASHBOARD This is the class that glues everything. It can be created using org.dashbuilder.dsl.factory.dashboard.DashboardFactory. Note: All these classes have builders that can be used instead of the factory Finally, when the work is done you can export the dashboard using the class org.dashbuilder.dsl.serialization.DashboardExporter. -------------------------------------------------------------------------------- HELLO WORLD DASHBOARD Let’s now, create a Hello world using our API: REQUIREMENTS 1. Java 11+ 2. Maven 3. Docker or podman STEPS 1. Create a maven project in your favorite IDE or use the following command: View the code on . Make sure to update it with the properties, dependencies, and build the section from this pom.xml: View the code on . 2. Create the class PopulationDashboard.java in package org.kie.dashbuilder with the content as seen below; It creates a dashboard and exports it to a ZIP file. View the code on . 3. On the project root, create the following directory structure. Notice that dashboards are a directory. View the code on . Make sure Dockerfile has the content as shown below View the code on . This is how the project structure should look like: View the code on . 4. Run PopulationDashboard.java in your IDE or using the command mvn clean install exec:java. After it runs the file population.zip is generated in dashbuilder-runtime/dashboards directory. This is the Dashboard you created: To visualize the dashboard you must: 1. Build the image. Inside dashbuilder-runtime directory build the image using the following docker/podman command &gt; docker build -t dashbuilder-dev . It will download DashBuilder runtime, so it may take a while. You must run this command once. 2. Then run the image: &gt; docker run -dp 8080:8080 -v ./dashboards:/tmp/dashbuilder/models:z &gt; dashbuilder-dev Once it is running you can access localhost:8080 and login as admin/admin. Every time you run PopulationDashboard Java class, the dashboard will be automatically updated. When the work is done, you can use the generated ZIP in production by using Dashbuilder Runtime in Static mode with the provided ZIP. CONCLUSION In this post, we introduced the Java API to create dashboards for Dashbuilder. In the next post, we will introduce other data set types and data set lookup, so stay tuned! The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/A_K5iDugtWo" height="1" width="1" alt=""/&gt;</content><dc:creator>William Siqueira</dc:creator><feedburner:origLink>https://blog.kie.org/2021/04/building-dashboards-using-plain-java.html</feedburner:origLink></entry><entry><title>The GDB developer’s GNU Debugger tutorial, Part 1: Getting started with the debugger</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/qz8lVbVUQow/" /><category term="C" /><category term="Developer Tools" /><category term="Linux" /><category term="Open source" /><category term="C/C++ debugger" /><category term="debugging" /><category term="gdb" /><category term="GNU Debugger" /><author><name>Keith Seitz</name></author><id>https://developers.redhat.com/blog/?p=841207</id><updated>2021-04-30T07:00:33Z</updated><published>2021-04-30T07:00:33Z</published><content type="html">&lt;p&gt;This article is the first in a series demonstrating how to use the &lt;a target="_blank" rel="nofollow" href="https://www.gnu.org/software/gdb/"&gt;GNU Debugger (GDB)&lt;/a&gt; effectively to debug applications in &lt;a target="_blank" rel="nofollow" href="/topics/c"&gt;C and C++&lt;/a&gt;. If you have limited or no experience using GDB, this series will teach you how to debug your code more efficiently. If you are already a seasoned professional using GDB, perhaps you will discover something you haven&amp;#8217;t seen before.&lt;/p&gt; &lt;p&gt;In addition to providing developer tips and tricks for many GDB commands, future articles will also cover topics such as debugging optimized code, offline debugging (core files), and server-based sessions (&lt;em&gt;aka&lt;/em&gt; &lt;code&gt;gdbserver&lt;/code&gt;, used in container debugging).&lt;/p&gt; &lt;h2&gt;Why another GDB tutorial?&lt;/h2&gt; &lt;p&gt;The majority of GDB tutorials available on the web consist of little more than introductions to the basic &lt;code&gt;list&lt;/code&gt;, &lt;code&gt;break&lt;/code&gt;, &lt;code&gt;print&lt;/code&gt;, and &lt;code&gt;run&lt;/code&gt; commands. New GDB users just might as well read (or sing) the &lt;a target="_blank" rel="nofollow" href="https://www.gnu.org/music/gdb-song.html"&gt;official GDB Song&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;Instead of simply demonstrating a handful of useful commands, each article in this series will focus on one aspect of using GDB from the perspective of someone who develops GDB. I use GDB daily, and these tips and tricks are the ones that I (and many other advanced GDB users and developers) use to streamline our debugging sessions.&lt;/p&gt; &lt;p&gt;Because this is the first article in the series, allow me to follow the recommendation of the GDB Song and start at the very beginning: How to run GDB.&lt;/p&gt; &lt;h2&gt;Compiler options&lt;/h2&gt; &lt;p&gt;Let me get the (all-too-often-not-so) obvious out of the way: For the best debugging experience, build applications without optimization and with debugging information. That is trivial advice, but GDB&amp;#8217;s public freenode.net IRC channel (#gdb) sees these issues often enough that they warrant mentioning.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: Don&amp;#8217;t debug applications with optimization if you can avoid it. Watch for a future article on optimization.&lt;/p&gt; &lt;p&gt;Optimization can cause GDB to behave in surprising ways if you are not aware of what might be happening &amp;#8220;under the covers.&amp;#8221; I always use the C compiler option &lt;code&gt;-O0&lt;/code&gt; (that&amp;#8217;s the letter &lt;em&gt;O&lt;/em&gt; followed by the number zero) to build executables during the development cycle.&lt;/p&gt; &lt;p&gt;I also always have the toolchain emit debugging information. This is accomplished with the &lt;code&gt;-g&lt;/code&gt; option. Specifying the exact debug format is no longer necessary (or desirable); DWARF has been the default debugging information format on GNU/Linux for many years. So ignore advice to use &lt;code&gt;-ggdb&lt;/code&gt; or &lt;code&gt;-gdwarf-2&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The one specific option worth adding is &lt;code&gt;-g3&lt;/code&gt;, which tells the compiler to include debugging information about the macros (&lt;code&gt;#define FOO ...&lt;/code&gt;) used in your application. These macros may then be used in GDB just like any other symbol in your program.&lt;/p&gt; &lt;p&gt;In short, for the best debugging experience, use &lt;code&gt;-g3 -O0&lt;/code&gt; when compiling your code. Some environments (such as those using GNU autotools) set environment variables (&lt;code&gt;CFLAGS&lt;/code&gt; and &lt;code&gt;CXXFLAGS&lt;/code&gt;) that control the compiler&amp;#8217;s output. Check these flags to make sure that your invocations of the compiler enable the debugging environment you want.&lt;/p&gt; &lt;p&gt;For much more information about the impact of &lt;code&gt;-g&lt;/code&gt; and &lt;code&gt;-O&lt;/code&gt; on the debugging experience, see Alexander Oliva&amp;#8217;s treatise &lt;a target="_blank" rel="nofollow" href="https://www.fsfla.org/~lxoliva/#gOlogy"&gt;GCC gOlogy: Studying the Impact of Optimizations on Debugging&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Startup scripts&lt;/h2&gt; &lt;p&gt;Before we look at actually using GDB, something must be said about how GDB starts up and what script files it executes. Upon startup, GDB will execute the commands contained in a number of system and user script files. The location and order of execution of these files are as follows:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;code&gt;/etc/gdbinit&lt;/code&gt; (not on FSF GNU GDB): In many GNU/Linux distributions, including Fedora and &lt;a target="_blank" rel="nofollow" href="/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt;, GDB looks first for the system default initialization file and executes commands contained therein. On Red Hat-based systems, this file executes any script files (including &lt;a target="_blank" rel="nofollow" href="/blog/category/python/"&gt;Python&lt;/a&gt; scripts) installed in &lt;code&gt;/etc/gdbinit.d&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;code&gt;$HOME/.gdbinit&lt;/code&gt;: GDB will then read the user&amp;#8217;s global initialization script from the home directory, if this file exists.&lt;/li&gt; &lt;li&gt;&lt;code&gt;./.gdbinit&lt;/code&gt;: Finally, GDB will look for a startup script in the current directory. Think of this as an application-specific customization file where you can add per-project user-defined commands, pretty-printers, and other customizations.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;All of these startup files contain GDB commands to execute, but they may also include Python scripts as long as they are prefaced with the &lt;code&gt;python&lt;/code&gt; command, e.g., &lt;code&gt;python print('Hello from python!')&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;My &lt;code&gt;.gdbinit&lt;/code&gt; is actually quite simple. Its most essential lines enable command history so that GDB remembers a given number of commands that were executed from a previous session. This is analogous to the shell&amp;#8217;s history mechanism and &lt;code&gt;.bash_history&lt;/code&gt;. The entire file is:&lt;/p&gt; &lt;pre&gt;set pagination off set history save on set history expansion on&lt;/pre&gt; &lt;p&gt;The first line turns off GDB&amp;#8217;s built-in paging. The next line enables saving the history (to &lt;code&gt;~/.gdb_history&lt;/code&gt; by default), and the final line enables shell-style history expansion with the exclamation point (!) character. This option is normally disabled because the exclamation point is also a logical operator in C.&lt;/p&gt; &lt;p&gt;To prevent GDB from reading initialization files, give it the &lt;code&gt;--nx&lt;/code&gt; command-line option.&lt;/p&gt; &lt;h2&gt;Getting help in GDB&lt;/h2&gt; &lt;p&gt;There are several ways to get help using GDB, including extensive—if dry—&lt;a target="_blank" rel="nofollow" href="https://sourceware.org/gdb/documentation/"&gt;documentation&lt;/a&gt; explaining every little switch, knob, and feature.&lt;/p&gt; &lt;h3&gt;GDB community resources&lt;/h3&gt; &lt;p&gt;The community offers help to users in two places:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Via email: The &lt;a target="_blank" rel="nofollow" href="https://sourceware.org/mailman/listinfo/gdb/"&gt;GDB mailing list&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Via IRC: #gdb on &lt;a target="_blank" rel="nofollow" href="http://freenode.net"&gt;freenode.net&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;However, because this article is about &lt;em&gt;using&lt;/em&gt; GDB, the easiest way for users to get help with a command is to use GDB&amp;#8217;s built-in help system, discussed next.&lt;/p&gt; &lt;h3&gt;Accessing the help system&lt;/h3&gt; &lt;p&gt;Access GDB&amp;#8217;s built-in help system via the &lt;code&gt;help&lt;/code&gt; and &lt;code&gt;apropos&lt;/code&gt; commands. Don&amp;#8217;t know how to use the &lt;code&gt;printf&lt;/code&gt; command? Ask GDB:&lt;/p&gt; &lt;pre&gt;(gdb) help printf Formatted printing, like the C "printf" function. Usage: printf "format string", ARG1, ARG2, ARG3, ..., ARGN This supports most C printf format specifications, like %s, %d, etc. (gdb)&lt;/pre&gt; &lt;p&gt;&lt;code&gt;help&lt;/code&gt; accepts the name of any GDB command or option and outputs usage information for that command or option.&lt;/p&gt; &lt;p&gt;Like all GDB commands, the &lt;code&gt;help&lt;/code&gt; command supports tab completion. This is perhaps the most useful way to figure out what types of arguments many commands accept. For instance, entering &lt;code&gt;help show ar&lt;/code&gt; and pressing the tab key will prompt you for a completion:&lt;/p&gt; &lt;pre&gt;(gdb) help show ar architecture args arm (gdb) help show ar&lt;/pre&gt; &lt;p&gt;GDB leaves you at the command prompt ready to accept further refinement of the input. Adding &lt;code&gt;g&lt;/code&gt; to the command, followed by a tab, will complete to &lt;code&gt;help show args&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;(gdb) help show args Show argument list to give program being debugged when it is started. Follow this command with any number of args, to be passed to the program. (gdb)&lt;/pre&gt; &lt;p&gt;Don&amp;#8217;t know the exact name of the command you&amp;#8217;re looking for? Use the &lt;code&gt;apropos&lt;/code&gt; command to search the help system for specific terms. Think of it as grepping the built-in help.&lt;/p&gt; &lt;p&gt;Now that you know how and where to find help, we&amp;#8217;re ready to move on to starting GDB (finally).&lt;/p&gt; &lt;h2&gt;Starting GDB&lt;/h2&gt; &lt;p&gt;Unsurprisingly, GDB accepts a large number of command-line options to change its behavior, but the most basic way to start GDB is to pass the application&amp;#8217;s name to GDB on the command line:&lt;/p&gt; &lt;pre&gt;$ gdb myprogram &lt;span style="color: magenta;"&gt;GNU gdb (GDB) Red Hat Enterprise Linux 9.2-2.el8&lt;/span&gt; Copyright (C) 2020 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later &amp;#60;http://gnu.org/licenses/gpl.html&amp;#62; This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Type "show copying" and "show warranty" for details. This GDB was configured as "x86_64-pc-linux-gnu". Type "show configuration" for configuration details. For bug reporting instructions, please see: &amp;#60;https://www.gnu.org/software/gdb/bugs/&amp;#62;. Find the GDB manual and other documentation resources online at: &amp;#60;http://www.gnu.org/software/gdb/documentation/&amp;#62;. For help, type "help". Type "apropos word" to search for commands related to "word"... Reading symbols from /home/blog/myprogram... (gdb) &lt;/pre&gt; &lt;p&gt;GDB starts up, prints out some version information (GCC Toolset 10 shown), loads the program and its debug information, and displays copyright and help messages, ending with the command prompt, &lt;code&gt;(gdb)&lt;/code&gt;. GDB is now ready to accept input.&lt;/p&gt; &lt;h3&gt;Avoiding messages: The -q or &amp;#8211;quiet option&lt;/h3&gt; &lt;p&gt;I&amp;#8217;ve seen GDB&amp;#8217;s startup message thousands of times, so I suppress (or &amp;#8220;quiet&amp;#8221;) it with the &lt;code&gt;-q&lt;/code&gt; option:&lt;/p&gt; &lt;pre&gt;$ gdb -q myprogram Reading symbols from /home/blog/myprogram... (gdb) &lt;/pre&gt; &lt;p&gt;That&amp;#8217;s much less to read. If you are really new to GDB, you might find the full startup messaging useful or soothing, but after a while, you&amp;#8217;ll also alias &lt;code&gt;gdb&lt;/code&gt; in your shell to &lt;code&gt;gdb -q&lt;/code&gt;. If you do need the suppressed information, use the &lt;code&gt;-v&lt;/code&gt; command-line option or the &lt;code&gt;show version&lt;/code&gt; command.&lt;/p&gt; &lt;h3&gt;Passing arguments: The &amp;#8211;args option&lt;/h3&gt; &lt;p&gt;Programs often require command-line arguments. GDB offers multiple ways to pass these to your program (or &amp;#8220;inferior,&amp;#8221; in GDB parlance). The two most useful ways are to pass application arguments via the &lt;code&gt;run&lt;/code&gt; command or at startup via the &lt;code&gt;--args&lt;/code&gt; command-line option. If your application is normally started with &lt;code&gt;myprogram 1 2 3 4&lt;/code&gt;, simply preface this with &lt;code&gt;gdb -q --args&lt;/code&gt; and GDB will remember how your application should be run:&lt;/p&gt; &lt;pre&gt;$ gdb -q --args myprogram 1 2 3 4 Reading symbols from &lt;span style="color: green;"&gt;/home/blog/myprogram&lt;/span&gt;... (gdb) show args Argument list to give program being debugged when it is started is "1 2 3 4". (gdb) run Starting program: /home/blog/myprogram 1 2 3 4 [Inferior 1 (process 1596525) exited normally] $ &lt;/pre&gt; &lt;h3&gt;Attaching to a running process: The &amp;#8211;pid option&lt;/h3&gt; &lt;p&gt;If an application is already running and gets &amp;#8220;stuck,&amp;#8221; you might want to look inside to find out why. Just give GDB the process ID of your application with &lt;code&gt;--pid&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;$ sleep 100000 &amp;#38; [1] 1591979 $ gdb -q --pid 1591979 Attaching to process 1591979 Reading symbols from &lt;span style="color: green;"&gt;/usr/bin/sleep&lt;/span&gt;... Reading symbols from &lt;span style="color: green;"&gt;.gnu_debugdata for /usr/bin/sleep&lt;/span&gt;... (No debugging symbols found in &lt;span style="color: green;"&gt;.gnu_debugdata for /usr/bin/sleep&lt;/span&gt;) Reading symbols from &lt;span style="color: green;"&gt;/lib64/libc.so.6&lt;/span&gt;... Reading symbols from &lt;span style="color: green;"&gt;/usr/lib/debug/usr/lib64/libc-2.31.so.debug&lt;/span&gt;... Reading symbols from &lt;span style="color: green;"&gt;/lib64/ld-linux-x86-64.so.2&lt;/span&gt;... Reading symbols from &lt;span style="color: green;"&gt;/usr/lib/debug/usr/lib64/ld-2.31.so.debug&lt;/span&gt;... &lt;span style="color: #0000cc;"&gt;0x00007fc421d5ef98&lt;/span&gt; in &lt;span style="color: #bbbb00;"&gt;__GI___clock_nanosleep&lt;/span&gt; (&lt;span style="color: #00bbbb;"&gt;requested_time=requested_time@entry&lt;/span&gt;=0, &lt;span style="color: #00bbbb;"&gt;remaining=remaining@entry&lt;/span&gt;=0x0) at &lt;span style="color: green;"&gt;../sysdeps/unix/sysv/linux/clock_nanosleep.c&lt;/span&gt;:28 28 &lt;span style="color: #6666dd;"&gt;return&lt;/span&gt; &lt;span style="color: grey;"&gt;SYSCALL_CANCEL&lt;/span&gt; &lt;span style="color: red;"&gt;(&lt;/span&gt;nanosleep&lt;span style="color: red;"&gt;,&lt;/span&gt; requested_time&lt;span style="color: red;"&gt;,&lt;/span&gt; remaining&lt;span style="color: red;"&gt;)&lt;/span&gt; (gdb) &lt;/pre&gt; &lt;p&gt;With this option, GDB automatically loads symbols for programs that have build ID information, such as distribution-supplied packages, and interrupts the program so that you can interact with it. Look for more on how and where GDB finds debug information in a future article.&lt;/p&gt; &lt;h3&gt;Following up on a failure: The &amp;#8211;core option&lt;/h3&gt; &lt;p&gt;If your process aborted and dumped core, use the &lt;code&gt;--core&lt;/code&gt; option to tell GDB to load the core file. If the core file contains the build ID of the aborted process, GDB automatically loads that binary and its debugging information if it can. Most developers, however, need to pass an executable to GDB with this option:&lt;/p&gt; &lt;pre&gt;$ ./abort-me Aborted (core dumped) $ gdb -q abort-me --core core.2127239 Reading symbols from &lt;span style="color: green;"&gt;abort-me&lt;/span&gt;... [New LWP 2127239] Core was generated by `./abort-me'. Program terminated with signal SIGABRT, Aborted. #0 &lt;span style="color: #bbbb00;"&gt;__GI_raise&lt;/span&gt; (&lt;span style="color: #00bbbb;"&gt;sig=sig@entry&lt;/span&gt;=6) at &lt;span style="color: green;"&gt;../sysdeps/unix/sysv/linux/raise.c&lt;/span&gt;:50 50 &lt;span style="color: #6666dd;"&gt;return&lt;/span&gt; ret&lt;span style="color: red;"&gt;;&lt;/span&gt; (gdb) &lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Tip&lt;/strong&gt;: Can&amp;#8217;t find a core file? On GNU/Linux systems using systemd, check &lt;code&gt;ulimit -c&lt;/code&gt; to see whether the shell is preventing programs from creating core files. If the value is &lt;code&gt;unlimited&lt;/code&gt;, use &lt;code&gt;coredumpctl&lt;/code&gt; to find the core file. Alternatively, run &lt;code&gt;sysctl -w kernel.core_pattern=core&lt;/code&gt; to configure systemd to output core files named &lt;code&gt;core.&lt;/code&gt;&lt;em&gt;&lt;code&gt;PID&lt;/code&gt;&lt;/em&gt;, as I have for the previous example.&lt;/p&gt; &lt;h3&gt;Expedited command execution: The &amp;#8211;ex, &amp;#8211;iex, &amp;#8211;x, and &amp;#8211;batch options&lt;/h3&gt; &lt;p&gt;I often run GDB commands repeatedly from the shell to test for problems or run scripts. These command-line options help facilitate that. Most users will use (multiple) &lt;code&gt;--ex&lt;/code&gt; arguments to specify commands to run at startup to recreate a debugging session, e.g., &lt;code&gt;gdb -ex "break &lt;/code&gt;&lt;em&gt;&lt;code&gt;some_function&lt;/code&gt;&lt;/em&gt;&lt;code&gt; if &lt;/code&gt;&lt;em&gt;&lt;code&gt;arg1&lt;/code&gt;&lt;/em&gt;&lt;code&gt; == nullptr" -ex r &lt;/code&gt;&lt;em&gt;&lt;code&gt;myprogram&lt;/code&gt;&lt;/em&gt;.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;--ex &lt;/code&gt;&lt;em&gt;&lt;code&gt;CMD&lt;/code&gt;&lt;/em&gt; runs the GDB command &lt;em&gt;&lt;code&gt;CMD&lt;/code&gt;&lt;/em&gt; after the program (and debug information) is loaded. &lt;code&gt;--iex&lt;/code&gt; does the same, but executes &lt;em&gt;&lt;code&gt;CMD&lt;/code&gt; before&lt;/em&gt; the specified program is loaded.&lt;/li&gt; &lt;li&gt;&lt;code&gt;-x &lt;/code&gt;&lt;em&gt;&lt;code&gt;FILE&lt;/code&gt;&lt;/em&gt; executes GDB commands from &lt;em&gt;&lt;code&gt;FILE&lt;/code&gt;&lt;/em&gt; after the program is loaded and &lt;code&gt;--ex&lt;/code&gt; commands execute. I use this option most often if I need a lot of &lt;code&gt;--ex&lt;/code&gt; arguments to reproduce a specific debugging session.&lt;/li&gt; &lt;li&gt;&lt;code&gt;--batch&lt;/code&gt; causes GDB to exit immediately at the first command prompt; i.e., after all commands or scripts have run. Note that &lt;code&gt;--batch&lt;/code&gt; will silence even more output than &lt;code&gt;-q&lt;/code&gt; to facilitate using GDB in scripts:&lt;/li&gt; &lt;/ul&gt; &lt;pre&gt;$ # All commands complete without error $ gdb -batch -x hello.gdb myprogram Reading symbols from &lt;span style="color: green;"&gt;myprogram&lt;/span&gt;... hello $ echo $? 0 $ # Command raises an exception $ gdb -batch -ex "set foo bar" No symbol "foo" in current context. $ echo $? 1 $ # Demonstrate the order of script execution $ gdb -x hello.gdb -iex 'echo before\n' -ex 'echo after\n' simple &lt;span style="color: magenta;"&gt;GNU gdb (GDB) Red Hat Enterprise Linux 9.2-2.el8&lt;/span&gt; Copyright (C) 2020 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later &amp;#60;http://gnu.org/licenses/gpl.html&amp;#62; This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Type "show copying" and "show warranty" for details. This GDB was configured as "x86_64-redhat-linux-gnu". Type "show configuration" for configuration details. For bug reporting instructions, please see: &amp;#60;https://www.gnu.org/software/gdb/bugs/&amp;#62;. Find the GDB manual and other documentation resources online at: &amp;#60;http://www.gnu.org/software/gdb/documentation/&amp;#62;. For help, type "help". Type "apropos word" to search for commands related to "word"... before Reading symbols from &lt;span style="color: green;"&gt;simple&lt;/span&gt;... hello after (gdb) &lt;/pre&gt; &lt;h2&gt;Next up&lt;/h2&gt; &lt;p&gt;In this article, I&amp;#8217;ve shared details about how GDB starts up, reads scripts (and when it reads scripts), and several startup options commonly used by advanced GDB users.&lt;/p&gt; &lt;p&gt;In the next article, I will take a small detour to explain what debugging information is, how to inspect it, where GDB looks for it, and how to install it in distribution-supplied packages.&lt;/p&gt; &lt;p&gt;Do you have a suggestion or tip related to GDB scripts or startup, or a suggestion for a future topic about how to use GDB? Leave a comment on this article and share your idea with us.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F30%2Fthe-gdb-developers-gnu-debugger-tutorial-part-1-getting-started-with-the-debugger%2F&amp;#38;linkname=The%20GDB%20developer%E2%80%99s%20GNU%20Debugger%20tutorial%2C%20Part%201%3A%20Getting%20started%20with%20the%20debugger" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F30%2Fthe-gdb-developers-gnu-debugger-tutorial-part-1-getting-started-with-the-debugger%2F&amp;#38;linkname=The%20GDB%20developer%E2%80%99s%20GNU%20Debugger%20tutorial%2C%20Part%201%3A%20Getting%20started%20with%20the%20debugger" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F30%2Fthe-gdb-developers-gnu-debugger-tutorial-part-1-getting-started-with-the-debugger%2F&amp;#38;linkname=The%20GDB%20developer%E2%80%99s%20GNU%20Debugger%20tutorial%2C%20Part%201%3A%20Getting%20started%20with%20the%20debugger" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F30%2Fthe-gdb-developers-gnu-debugger-tutorial-part-1-getting-started-with-the-debugger%2F&amp;#38;linkname=The%20GDB%20developer%E2%80%99s%20GNU%20Debugger%20tutorial%2C%20Part%201%3A%20Getting%20started%20with%20the%20debugger" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F30%2Fthe-gdb-developers-gnu-debugger-tutorial-part-1-getting-started-with-the-debugger%2F&amp;#38;linkname=The%20GDB%20developer%E2%80%99s%20GNU%20Debugger%20tutorial%2C%20Part%201%3A%20Getting%20started%20with%20the%20debugger" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F30%2Fthe-gdb-developers-gnu-debugger-tutorial-part-1-getting-started-with-the-debugger%2F&amp;#38;linkname=The%20GDB%20developer%E2%80%99s%20GNU%20Debugger%20tutorial%2C%20Part%201%3A%20Getting%20started%20with%20the%20debugger" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F30%2Fthe-gdb-developers-gnu-debugger-tutorial-part-1-getting-started-with-the-debugger%2F&amp;#38;linkname=The%20GDB%20developer%E2%80%99s%20GNU%20Debugger%20tutorial%2C%20Part%201%3A%20Getting%20started%20with%20the%20debugger" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F30%2Fthe-gdb-developers-gnu-debugger-tutorial-part-1-getting-started-with-the-debugger%2F&amp;#038;title=The%20GDB%20developer%E2%80%99s%20GNU%20Debugger%20tutorial%2C%20Part%201%3A%20Getting%20started%20with%20the%20debugger" data-a2a-url="https://developers.redhat.com/blog/2021/04/30/the-gdb-developers-gnu-debugger-tutorial-part-1-getting-started-with-the-debugger/" data-a2a-title="The GDB developer’s GNU Debugger tutorial, Part 1: Getting started with the debugger"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/30/the-gdb-developers-gnu-debugger-tutorial-part-1-getting-started-with-the-debugger/"&gt;The GDB developer&amp;#8217;s GNU Debugger tutorial, Part 1: Getting started with the debugger&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/qz8lVbVUQow" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;This article is the first in a series demonstrating how to use the GNU Debugger (GDB) effectively to debug applications in C and C++. If you have limited or no experience using GDB, this series will teach you how to debug your code more efficiently. If you are already a seasoned professional using GDB, perhaps [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/30/the-gdb-developers-gnu-debugger-tutorial-part-1-getting-started-with-the-debugger/"&gt;The GDB developer&amp;#8217;s GNU Debugger tutorial, Part 1: Getting started with the debugger&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/04/30/the-gdb-developers-gnu-debugger-tutorial-part-1-getting-started-with-the-debugger/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">841207</post-id><dc:creator>Keith Seitz</dc:creator><dc:date>2021-04-30T07:00:33Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/04/30/the-gdb-developers-gnu-debugger-tutorial-part-1-getting-started-with-the-debugger/</feedburner:origLink></entry><entry><title>Detecting memory management bugs with GCC 11, Part 1: Understanding dynamic allocation</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/n7mImlVVF8s/" /><category term="C" /><category term="Linux" /><category term="Open source" /><category term="Security" /><category term="Attribute malloc" /><category term="debugging" /><category term="Dynamic allocation" /><category term="GCC 11" /><author><name>Martin Sebor</name></author><id>https://developers.redhat.com/blog/?p=871847</id><updated>2021-04-30T07:00:24Z</updated><published>2021-04-30T07:00:24Z</published><content type="html">&lt;p&gt;Memory management bugs are among the hardest to find in &lt;a target="_blank" rel="nofollow" href="/topics/c/"&gt;C and C++&lt;/a&gt; programs, and are a favorite target of exploits. These errors are difficult to debug because they involve three distinct sites in a program that are often far apart and obscured by the use of pointers: memory allocation, the use of the allocated memory, and the release of memory back to the system by deallocation. In this two-part article, we&amp;#8217;ll look at &lt;a target="_blank" rel="nofollow" href="https://gcc.gnu.org/"&gt;GNU Compiler Collection (GCC) 11&lt;/a&gt; enhancements that help detect the subset of these bugs that affect dynamically allocated memory. The enhancements discussed here have been made to the GCC core. Related improvements to the GCC static analyzer are covered by David Malcolm in his article &lt;a target="_blank" rel="nofollow" href="/blog/2021/01/28/static-analysis-updates-in-gcc-11"&gt;Static analysis updates in GCC 11&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Throughout this article, I include links to the code examples on &lt;a target="_blank" rel="nofollow" href="https://godbolt.org"&gt;Compiler Explorer&lt;/a&gt; for those who would like to experiment. You will find the links above the source code of each example.&lt;/p&gt; &lt;h2&gt;Overview of memory allocation policies&lt;/h2&gt; &lt;p&gt;Let&amp;#8217;s start with a quick breakdown of the major kinds of memory management bugs. C and C++ outline four broad kinds of storage allocation policies:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Automatic&lt;/strong&gt;: This policy allocates objects on the stack of a function. With the notable exception of the nonstandard, discouraged, yet still widely used &lt;code&gt;alloca()&lt;/code&gt; function, automatic objects are allocated when they are declared. They are then commonly referred to by name, except when they are passed to other functions by reference or when pointers are used to point to the elements of arrays. As the name implies, automatic objects are deallocated automatically, at the end of the block in which they are declared. Objects allocated by &lt;code&gt;alloca()&lt;/code&gt; are deallocated on function return.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dynamic&lt;/strong&gt;: This policy allocates objects on the heap by an explicit call to an allocation function. To avoid memory exhaustion, dynamically allocated objects must be deallocated by an explicit call to a corresponding deallocation function.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Static&lt;/strong&gt;: This policy allocates named objects that last for the duration of a program. They never go out of scope and so they are never deallocated during the program&amp;#8217;s execution.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Thread&lt;/strong&gt;: Like static, but limited in duration to a single thread of execution. Objects in this policy are automatically deallocated at the termination of the thread in which they are created.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Of these four policies, the one that represents the most common, but also the most insidious, class of problems is &lt;em&gt;dynamic allocation&lt;/em&gt;. This form of allocation is the subject of this two-part article. To be sure, plenty of bugs also have to do with automatic storage (think about uninitialized reads, or accessing a local variable after it has gone out of scope through a pointer obtained while it was still live), but we will talk about those another time.&lt;/p&gt; &lt;h2&gt;New command-line options in GCC 11&lt;/h2&gt; &lt;p&gt;Before diving into the details of the dynamic memory management bugs that GCC 11 can detect, let&amp;#8217;s quickly summarize the command-line options that control detection. All the options are enabled by default. Although they perform best with optimization enabled, they don&amp;#8217;t require it.&lt;/p&gt; &lt;p&gt;GCC 11 provides two new options and significantly enhances one that has been available for several releases:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;&lt;a target="_blank" rel="nofollow" href="https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmismatched-dealloc"&gt;-Wmismatched-dealloc&lt;/a&gt;&lt;/code&gt; controls warnings about mismatches between calls to general memory allocation and deallocation functions. This option is new in GCC 11.&lt;/li&gt; &lt;li&gt;&lt;code&gt;&lt;a target="_blank" rel="nofollow" href="https://gcc.gnu.org/onlinedocs/gcc/C_002b_002b-Dialect-Options.html#index-Wmismatched-new-delete"&gt;-Wmismatched-new-delete&lt;/a&gt;&lt;/code&gt; controls warnings about mismatches between calls specifically to &lt;code&gt;operator new()&lt;/code&gt; and &lt;code&gt;operator delete()&lt;/code&gt; in C++. This option is also new in GCC 11.&lt;/li&gt; &lt;li&gt;&lt;code&gt;&lt;a target="_blank" rel="nofollow" href="https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wfree-nonheap-object"&gt;-Wfree-nonheap-object&lt;/a&gt;&lt;/code&gt; controls warnings about invalid attempts to deallocate pointers that were not returned by dynamic allocation functions. This option has been enhanced in GCC 11.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Dynamic memory management functions&lt;/h2&gt; &lt;p&gt;The best known dynamic memory management functions in C are &lt;code&gt;calloc()&lt;/code&gt;, &lt;code&gt;malloc()&lt;/code&gt;, &lt;code&gt;realloc()&lt;/code&gt;, and &lt;code&gt;free()&lt;/code&gt;. But they are not the only ones. In addition to these C89 functions, C99 introduced &lt;code&gt;aligned_alloc()&lt;/code&gt;. POSIX adds a few of its own allocation functions to the mix, including &lt;code&gt;strdup()&lt;/code&gt;, &lt;code&gt;strndup()&lt;/code&gt;, and &lt;code&gt;tempnam()&lt;/code&gt;, among others. C library implementations often provide their own extensions. For instance, FreeBSD, Linux, and Solaris all define a function named &lt;a target="_blank" rel="nofollow" href="https://www.freebsd.org/cgi/man.cgi?query=reallocarray"&gt;&lt;code&gt;reallocarray()&lt;/code&gt;&lt;/a&gt; that&amp;#8217;s a hybrid between &lt;code&gt;calloc()&lt;/code&gt; and &lt;code&gt;realloc()&lt;/code&gt;. The pointer returned by all these allocation functions must be passed to &lt;code&gt;free()&lt;/code&gt; to be deallocated.&lt;/p&gt; &lt;p&gt;Besides functions that dynamically allocate raw memory, several other standard APIs allocate and deallocate other resources. For instance, &lt;code&gt;fopen()&lt;/code&gt;, &lt;code&gt;fdopen()&lt;/code&gt;, and the POSIX &lt;a target="_blank" rel="nofollow" href="https://pubs.opengroup.org/onlinepubs/9699919799/functions/open_memstream.html"&gt;&lt;code&gt;open_memstream()&lt;/code&gt;&lt;/a&gt; create and initialize &lt;code&gt;FILE&lt;/code&gt; objects that must then be disposed of by calling &lt;code&gt;fclose()&lt;/code&gt;; the &lt;code&gt;popen()&lt;/code&gt; function also creates &lt;code&gt;FILE&lt;/code&gt;s, but those must be closed by calling &lt;code&gt;pclose()&lt;/code&gt;. Similarly, the POSIX &lt;a target="_blank" rel="nofollow" href="https://pubs.opengroup.org/onlinepubs/9699919799/functions/newlocale.html"&gt;&lt;code&gt;newlocale()&lt;/code&gt;&lt;/a&gt; and &lt;a target="_blank" rel="nofollow" href="https://pubs.opengroup.org/onlinepubs/9699919799/functions/duplocale.html"&gt;&lt;code&gt;duplocale()&lt;/code&gt;&lt;/a&gt; functions create locales that must be destroyed by calling &lt;a target="_blank" rel="nofollow" href="https://pubs.opengroup.org/onlinepubs/9699919799/functions/freelocale.html"&gt;&lt;code&gt;freelocale()&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Finally, many third-party libraries and programs define their own functions either to allocate raw memory or to initialize objects of various types that reside in allocated memory. These functions usually return pointers to the objects to their clients. The simplest of these can be deallocated directly by calling &lt;code&gt;free()&lt;/code&gt;, but most APIs rely on their clients to destroy and deallocate objects by &amp;#8220;returning&amp;#8221; them to the appropriate deallocation function.&lt;/p&gt; &lt;p&gt;All these groups of APIs share a common theme: the allocation function in each group returns a pointer that&amp;#8217;s used to access the object and, importantly, that must eventually be passed to the appropriate deallocation function in the same group. The result of &lt;code&gt;malloc()&lt;/code&gt; must be passed to &lt;code&gt;free()&lt;/code&gt;, and that of &lt;code&gt;fopen()&lt;/code&gt; to &lt;code&gt;fclose()&lt;/code&gt;. Therefore, passing the result of &lt;code&gt;fopen()&lt;/code&gt; to &lt;code&gt;free()&lt;/code&gt; is a bug, as is calling &lt;code&gt;fclose()&lt;/code&gt; on a pointer returned from &lt;code&gt;malloc()&lt;/code&gt;. In addition, in C++, the result of a given form of &lt;code&gt;operator new()&lt;/code&gt;—either ordinary or array—must be deallocated by the corresponding form of &lt;code&gt;operator delete()&lt;/code&gt;, but not by calling &lt;code&gt;free()&lt;/code&gt; or &lt;code&gt;realloc()&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Matching allocations with deallocations&lt;/h2&gt; &lt;p&gt;Calling the wrong deallocation function to release a resource allocated by an allocation function from a different group usually leads to memory corruption. The call might crash right there and then, sometimes even with a helpful message, or might return to the caller and crash sometime later, in an area unrelated to the invalid call. Or the deallocation function might not crash at all but instead overwrite some data, leading to unpredictable behavior at some later point. Naturally, we would like to detect and prevent these bugs, not just before they make it into a product release, but ideally during code development before they are committed into the code base. The challenge is how to let our tools—compilers or static analyzers—know which functions must be used to deallocate each of the objects allocated by other functions.&lt;/p&gt; &lt;p&gt;For a subset of standard functions, the semantics and the associations can be and often are baked into the tools themselves. For example, GCC knows the effects of the standard C and C++ dynamic memory management functions and which ones go with which, but it doesn&amp;#8217;t have the same knowledge of &lt;code&gt;&amp;#60;stdio.h&amp;#62;&lt;/code&gt; functions such as &lt;code&gt;fopen()&lt;/code&gt; and &lt;code&gt;fclose()&lt;/code&gt;, or about implementation-defined extensions. Additionally, GCC knows nothing about user-defined functions.&lt;/p&gt; &lt;h2&gt;Attribute malloc&lt;/h2&gt; &lt;p&gt;Enter &lt;a target="_blank" rel="nofollow" href="https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attributes.html#index-malloc-function-attribute"&gt;attribute &lt;code&gt;malloc&lt;/code&gt;&lt;/a&gt;, or more accurately, an enhancement to it implemented in GCC 11. In its traditional form, the attribute takes no arguments and simply lets GCC know that the function it applies to returns dynamically allocated memory like &lt;code&gt;malloc()&lt;/code&gt;. This property is used by GCC to make aliasing assumptions about the contents of the returned memory and emit more efficient code. GCC 11 extends attribute &lt;code&gt;malloc&lt;/code&gt; to take one or two arguments: the name of the deallocation function to call to release the allocated object and, optionally, the positional argument number to which the pointer must be passed. The same allocation function can be paired with any number of deallocation functions. For example, the following declarations designate &lt;code&gt;fclose()&lt;/code&gt; as the deallocator for &lt;code&gt;fopen()&lt;/code&gt;, &lt;code&gt;fdopen()&lt;/code&gt;, &lt;code&gt;fmemopen()&lt;/code&gt;, and &lt;code&gt;tmpfile()&lt;/code&gt;, and &lt;code&gt;pclose()&lt;/code&gt; as the only deallocator for &lt;code&gt;popen()&lt;/code&gt;.&lt;/p&gt; &lt;pre&gt;int &lt;b&gt;fclose&lt;/b&gt; (FILE*); int &lt;b&gt;pclose&lt;/b&gt; (FILE*); __attribute__ ((malloc (&lt;strong&gt;fclose&lt;/strong&gt;, 1)))) FILE* &lt;b&gt;fdopen&lt;/b&gt; (int); __attribute__ ((malloc (&lt;strong&gt;fclose&lt;/strong&gt;, 1)))) FILE* &lt;b&gt;fopen&lt;/b&gt; (const char*, const char*); __attribute__ ((malloc (&lt;strong&gt;fclose&lt;/strong&gt;, 1)))) FILE* &lt;b&gt;fmemopen&lt;/b&gt; (void *, size_t, const char *); __attribute__ ((malloc (&lt;strong&gt;pclose&lt;/strong&gt;, 1)))) FILE* &lt;b&gt;popen&lt;/b&gt; (const char*, const char*); __attribute__ ((malloc (&lt;strong&gt;fclose&lt;/strong&gt;, 1)))) FILE* &lt;b&gt;tmpfile&lt;/b&gt; (void);&lt;/pre&gt; &lt;p&gt;Ideally, the declarations in &lt;code&gt;&amp;#60;stdio.h&amp;#62;&lt;/code&gt; and other C library headers would be decorated with the attribute&lt;code&gt;malloc&lt;/code&gt; as just shown. A patch for glibc on Linux was submitted but hasn&amp;#8217;t been approved yet. Until that happens, you can add the previous declarations to your own header to enable the same detection. The full patch can also be downloaded &lt;a target="_blank" rel="nofollow" href="https://sourceware.org/pipermail/libc-alpha/2021-January/121527.html"&gt;from sourceware.org&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Both GCC proper and the integrated static analyzer make use of the new attribute to issue similar warnings. The static analyzer detects a broader set of problems at the cost of increased compilation time.&lt;/p&gt; &lt;h2&gt;Detecting mismatched deallocations&lt;/h2&gt; &lt;p&gt;The new attribute &lt;code&gt;malloc&lt;/code&gt;is used by a number of warnings in GCC 11 to detect various memory management bugs. The &lt;code&gt;-Wmismatched-dealloc&lt;/code&gt; option controls warnings about deallocation calls with arguments returned from mismatched allocation functions. For example, given the declarations in the previous section, the call to &lt;code&gt;fclose()&lt;/code&gt; in the following function is diagnosed because the pointer passed to it was returned from an allocation function that&amp;#8217;s not associated with it: &lt;code&gt;popen()&lt;/code&gt;. The &lt;a target="_blank" rel="nofollow" href="https://godbolt.org/z/Wqfx93"&gt;&lt;code&gt;popen_pclose&lt;/code&gt;&lt;/a&gt; example shows how this works:&lt;/p&gt; &lt;pre&gt;void test_popen_fclose (void) { FILE *f = popen ("/bin/ls"); // use f fclose (f); } &lt;/pre&gt; &lt;p&gt;The compiler warning is:&lt;/p&gt; &lt;pre&gt;In function '&lt;b&gt;test_popen_fclose&lt;/b&gt;': &lt;span style="color: #ff00ff;"&gt;&lt;b&gt;warning&lt;/b&gt;&lt;/span&gt;: '&lt;b&gt;fclose&lt;/b&gt;' called on pointer returned from a mismatched allocation function [&lt;span style="color: #ff00ff;"&gt;&lt;b&gt;-Wmismatched-dealloc&lt;/b&gt;&lt;/span&gt;] 21 | &lt;b&gt;fclose (f);&lt;/b&gt; | &lt;b&gt;^~~~~~~~~~&lt;/b&gt; &lt;span style="color: #33cccc;"&gt;&lt;b&gt;note&lt;/b&gt;&lt;/span&gt;: returned from '&lt;b&gt;popen&lt;/b&gt;' 19 | &lt;b&gt;FILE *f = popen ("/bin/ls", "r");&lt;/b&gt; | &lt;b&gt;^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~&lt;/b&gt; &lt;/pre&gt; &lt;h2&gt;Conclusion to Part 1&lt;/h2&gt; &lt;p&gt;Look for the second half of this article, where I will describe more options for detecting dynamic allocation bugs. I will conclude with situations that can lead to false positive or false negative identifications.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F30%2Fdetecting-memory-management-bugs-with-gcc-11-part-1-understanding-dynamic-allocation%2F&amp;#38;linkname=Detecting%20memory%20management%20bugs%20with%20GCC%2011%2C%20Part%201%3A%20Understanding%20dynamic%20allocation" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F30%2Fdetecting-memory-management-bugs-with-gcc-11-part-1-understanding-dynamic-allocation%2F&amp;#38;linkname=Detecting%20memory%20management%20bugs%20with%20GCC%2011%2C%20Part%201%3A%20Understanding%20dynamic%20allocation" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F30%2Fdetecting-memory-management-bugs-with-gcc-11-part-1-understanding-dynamic-allocation%2F&amp;#38;linkname=Detecting%20memory%20management%20bugs%20with%20GCC%2011%2C%20Part%201%3A%20Understanding%20dynamic%20allocation" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F30%2Fdetecting-memory-management-bugs-with-gcc-11-part-1-understanding-dynamic-allocation%2F&amp;#38;linkname=Detecting%20memory%20management%20bugs%20with%20GCC%2011%2C%20Part%201%3A%20Understanding%20dynamic%20allocation" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F30%2Fdetecting-memory-management-bugs-with-gcc-11-part-1-understanding-dynamic-allocation%2F&amp;#38;linkname=Detecting%20memory%20management%20bugs%20with%20GCC%2011%2C%20Part%201%3A%20Understanding%20dynamic%20allocation" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F30%2Fdetecting-memory-management-bugs-with-gcc-11-part-1-understanding-dynamic-allocation%2F&amp;#38;linkname=Detecting%20memory%20management%20bugs%20with%20GCC%2011%2C%20Part%201%3A%20Understanding%20dynamic%20allocation" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F30%2Fdetecting-memory-management-bugs-with-gcc-11-part-1-understanding-dynamic-allocation%2F&amp;#38;linkname=Detecting%20memory%20management%20bugs%20with%20GCC%2011%2C%20Part%201%3A%20Understanding%20dynamic%20allocation" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F30%2Fdetecting-memory-management-bugs-with-gcc-11-part-1-understanding-dynamic-allocation%2F&amp;#038;title=Detecting%20memory%20management%20bugs%20with%20GCC%2011%2C%20Part%201%3A%20Understanding%20dynamic%20allocation" data-a2a-url="https://developers.redhat.com/blog/2021/04/30/detecting-memory-management-bugs-with-gcc-11-part-1-understanding-dynamic-allocation/" data-a2a-title="Detecting memory management bugs with GCC 11, Part 1: Understanding dynamic allocation"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/30/detecting-memory-management-bugs-with-gcc-11-part-1-understanding-dynamic-allocation/"&gt;Detecting memory management bugs with GCC 11, Part 1: Understanding dynamic allocation&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/n7mImlVVF8s" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Memory management bugs are among the hardest to find in C and C++ programs, and are a favorite target of exploits. These errors are difficult to debug because they involve three distinct sites in a program that are often far apart and obscured by the use of pointers: memory allocation, the use of the allocated [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/30/detecting-memory-management-bugs-with-gcc-11-part-1-understanding-dynamic-allocation/"&gt;Detecting memory management bugs with GCC 11, Part 1: Understanding dynamic allocation&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/04/30/detecting-memory-management-bugs-with-gcc-11-part-1-understanding-dynamic-allocation/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">871847</post-id><dc:creator>Martin Sebor</dc:creator><dc:date>2021-04-30T07:00:24Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/04/30/detecting-memory-management-bugs-with-gcc-11-part-1-understanding-dynamic-allocation/</feedburner:origLink></entry></feed>
